<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: R | Jason A. French]]></title>
  <link href="http://frenchja.github.com/blog/categories/r/atom.xml" rel="self"/>
  <link href="http://frenchja.github.com/"/>
  <updated>2014-07-03T13:20:49-07:00</updated>
  <id>http://frenchja.github.com/</id>
  <author>
    <name><![CDATA[Jason A. French]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Using R with MySQL Databases]]></title>
    <link href="http://frenchja.github.com/blog/2014/07/03/using-r-with-mysql-databases/"/>
    <updated>2014-07-03T10:58:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2014/07/03/using-r-with-mysql-databases</id>
    <content type="html"><![CDATA[<h2>Overview</h2>

<p>When I began using R, like most researchers I kept all my data in some combination
of R's native <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html">data.frame</a>
format or a <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> file that my analysis would continually read.
However, as I began to analyze big datasets at the <a href="https://sapa-project.org">SAPA Project</a>
and at <a href="http://insightdatascience.com/">Insight</a>,
I realized that there is a lot of value to instead keeping your data in a MySQL database.
This post will detail a few advantages of using a database to store data and run through a
basic example of using R to transfer data to MySQL.</p>

<!-- more -->


<h3>Memory Efficiency</h3>

<p>For example, relational databases are better at handling large datasets and are more efficient
at storing data than CSVs.
When reading data stored in a data.frame or CSV file, the data must all fit <em>in memory</em>.
All of your data may not fit easily in memory, as is the case if you have a
large dataset or are cursed with an older computers with &lt;= 4GB of RAM.
In these cases, every time you load your dataset or do an expensive operation
your computer will slow to a painful crawl as your hard drives grind while your operating system
switches to using <a href="http://en.wikipedia.org/wiki/Virtual_memory">virtual memory</a>, rather than RAM.
You can get around this by using random sampling to sample a smaller subset (or multiple subsets
if you want to bootstrap), but you can also use this technique on a SQL database. The advantage is that SQL databases
only load the data you're working with into the machine's RAM.</p>

<h3>Safety and Security</h3>

<p>Two additional reasons I prefer SQL databases are safety and security. Services such as
Amazon's RDS (i.e., Relational Database Service) offer frequent backups and easily replication
across machines. This means that if your machine dies, your data is safe and uncorrupted. If I were storing
data in an .Rdata file, I'd have to rely on my OS (e.g., Time Machine) or a cloud-based storage solution
(e.g., Dropbox or CrashPlan) to backup my data. Worse, if I were storing my data in an Excel database and Excel
crashed, I may risk losing years of progress.</p>

<p>Furthermore, I can give a collaborator access
to only needed portions of the data by locking down their SQL username to certain tables
and operations. By limiting access, we limit risk of corruption and overwriting years of progress.</p>

<h3>Scalable as your need grows</h3>

<p>Last, I find relational databases such as MySQL to be more scalable. In the case of using Amazon's RDS,
you can buy as much storage or bandwidth as you need to complete your analysis, dissertation,
or build your web app. If I need multiple nodes or machines to analyze the data, I don't need to keep a copy of
the data on each machine.  I simply pass the SQL authentication parameters to each machine and the
data is accessed from a central location.</p>

<p>So let's begin by setting up a <a href="http://aws.amazon.com/rds/free/">Free-Tier RDS service</a> on Amazon and then move some
data from R to the RDS database.</p>

<h2>Setting up an RDS Instance</h2>

<p>Amazon currently offers a Free Tier for their RDS storage.  It offers:</p>

<ul>
<li>Enough hours to run a DB Instance continuously each month,</li>
<li>20 GB of database storage,</li>
<li>10 million I/O operations, and</li>
<li>automated database backups.</li>
</ul>


<p>To setup an RDS instance:</p>

<ol>
<li>Sign into the AWS Management Console.</li>
<li>Select 'RDS' under the 'Database' group.</li>
<li>Click 'Launch DB Instance'.</li>
<li>Select 'MySQL' for the database engine.</li>
<li>Select 'No' when asked if you intend to use the database for production purposes. Accidentally selecting 'Yes' could result in hefty fees.</li>
<li>For 'Instance Class', select 'db.t1.micro', 'No' for 'Multi-AZ Deployment', and 20 GB for 'Allocated Storage'.</li>
<li>Click 'Next' and launch your RDS instance.</li>
</ol>


<p>At this point, you should have a blank RDS database with a master username and password combination. Be sure to test that you can connect - I suggest using <a href="http://www.sequelpro.com/">Sequel Pro</a>. <em>If you can't connect,
you may need to open up port 3306 in your EC2 Security Group</em>.</p>

<p>Click on your new RDS Instance to get the hostname to connect to and note the database name:</p>

<p><img class="center" src="/images/rds1.png"></p>

<p>Fire up Sequel Pro and test out your new database.  If you want to connect using SSL, you'll need the SSH key you generated when
you first created your EC2 instance (not covered here).</p>

<p><img class="center" src="/images/rds2.png"></p>

<h2>Installing the RMySQL package</h2>

<p>Previously, I found installing <a href="">RMySQL</a> to be very difficult when I was using MacPorts.  However,
having switched to <a href="http://brew.sh/">Homebrew</a> on my new machine, RMySQL found all my MySQL headers and libraries as
<a href="http://brew.sh/">Homebrew</a> uses the <code>/usr/local</code> directory to store your software, which is already in your BASH $PATH.</p>

<p>Before compiling RMySQL, first compile MySQL using Homebrew:</p>

<p>```bash Installing MySQL using Homebrew</p>

<h1>If you haven't already, install Homebrew:</h1>

<h1>ruby -e "$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)"</h1>

<h1>Install MySQL</h1>

<p>brew install mysql
```</p>

<p>```
==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/mysql-5.6.1
Already downloaded: /Library/Caches/Homebrew/mysql-5.6.19.mavericks.bottle.tar.gz
==> Pouring mysql-5.6.19.mavericks.bottle.tar.gz
==> Caveats
A "/etc/my.cnf" from another install may interfere with a Homebrew-built
server starting up correctly.</p>

<p>To connect:</p>

<pre><code>mysql -uroot
</code></pre>

<p>To have launchd start mysql at login:</p>

<pre><code>ln -sfv /usr/local/opt/mysql/*.plist ~/Library/LaunchAgents
</code></pre>

<p>Then to load mysql now:</p>

<pre><code>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist
</code></pre>

<p>Or, if you don't want/need launchctl, you can just run:</p>

<pre><code>mysql.server start
</code></pre>

<p>==> Summary
üç∫  /usr/local/Cellar/mysql/5.6.19: 9536 files, 338M</p>

<p>```</p>

<p>Next, compile RMySQL from source:</p>

<p><code>r Compiling RMySQL
install.packages('RMySQL', type = 'source')
</code></p>

<p>```
trying URL 'http://cran.rstudio.com/src/contrib/RMySQL_0.9-3.tar.gz'
Content type 'application/x-gzip' length 165363 bytes (161 Kb)</p>

<h1>opened URL</h1>

<p>downloaded 161 Kb</p>

<ul>
<li>installing <em>source</em> package ‚ÄòRMySQL‚Äô ...
<strong> package ‚ÄòRMySQL‚Äô successfully unpacked and MD5 sums checked
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for compress in -lz... yes
checking for getopt_long in -lc... yes
checking for mysql_init in -lmysqlclient... yes
checking for egrep... grep -E
checking for ANSI C header files...
rm: conftest.dSYM: is a directory
rm: conftest.dSYM: is a directory
yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking mysql.h usability... no
checking mysql.h presence... no
checking for mysql.h... no
checking /usr/local/include/mysql/mysql.h usability... yes
checking /usr/local/include/mysql/mysql.h presence... yes
checking for /usr/local/include/mysql/mysql.h... yes
configure: creating ./config.status
config.status: creating src/Makevars
</strong> libs
clang -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include/mysql -I/usr/local/include -I/usr/local/include/freetype2 -I/opt/X11/include    -fPIC  -Wall -mtune=core2 -g -O2  -c RS-DBI.c -o RS-DBI.o
clang -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include/mysql -I/usr/local/include -I/usr/local/include/freetype2 -I/opt/X11/include    -fPIC  -Wall -mtune=core2 -g -O2  -c RS-MySQL.c -o RS-MySQL.o
clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o -lmysqlclient -lz -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
installing to /Library/Frameworks/R.framework/Versions/3.1/Resources/library/RMySQL/libs
<strong> R
</strong> inst
<strong> preparing package for lazy loading
Creating a generic function for ‚Äòformat‚Äô from package ‚Äòbase‚Äô in package ‚ÄòRMySQL‚Äô
Creating a generic function for ‚Äòprint‚Äô from package ‚Äòbase‚Äô in package ‚ÄòRMySQL‚Äô
</strong> help
<strong>* installing help indices
</strong> building package indices
** testing if installed package can be loaded</li>
<li>DONE (RMySQL)
```</li>
</ul>


<h2>Transferring Existing Data to MySQL</h2>

<p>Having a database is useless unless we can easily convert our existing data.frames
into MySQL tables.  Let's try this using the <code>Thurstone</code> dataset from the Bill's <code>psych</code> package:</p>

<p>```r Moving Thurstone to RDS
library(RMySQL)
library(psych)
con &lt;- dbConnect(MySQL(),</p>

<pre><code>user = 'RDSUser',
password = 'YourPass',
host = 'RDS Host',
dbname='YourDB')
</code></pre>

<p>dbWriteTable(conn = con, name = 'Test', value = as.data.frame(Thurstone))
```</p>

<p>There! Now we have transferred out data.frame to our SQL database. Similarly, if
we want to read data <em>from</em> MySQL to R, we can use the <code>dbReadTable()</code> function,
which returns a data.frame.</p>

<p><img class="center" src="/images/rds3.png"></p>

<h2>Useful Packages for Fast DB Operations</h2>

<p>Ideally, we want to have one set of analysis code that is somewhat agnostic to <em>how</em>
the data are stored.  I suggest looking into <a href="http://had.co.nz/">Hadley Wickham's</a> new <a href="http://blog.rstudio.org/2014/01/17/introducing-dplyr/"><code>dplyr</code></a> package
(<a href="https://github.com/hadley/dplyr">Github</a>). <code>dplyr</code> is able to filter, sort, group, and summarize data
quickly whether it is stored in a data.frame, data.table, or database. Database operations may not always be as
fast as a data.table operation, but again, the advantage is that you don't need to feed all of your data into memory.</p>

<p>Hadley provides a few vignettes for getting used to <code>dplyr</code>:</p>

<p><code>r
vignette("introduction", package = "dplyr")
vignette("databases", package = "dplyr")
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fixing knitr: Formatting statistical output to 2 digits in R]]></title>
    <link href="http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits/"/>
    <updated>2014-04-25T16:53:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits</id>
    <content type="html"><![CDATA[<h3>Overview of reproducible research</h3>

<p>Reproducible research is a phrase that describes an academic paper or manuscript that contains the code and data in addition to what is usually published - the researcher's interpretation.  In doing so, the experimental design and method of analysis is easily replicated by unaffiliated labs and critiqued by reviewers as the full analysis used to produce the results is submitted along with the final paper.  One way of producing reproducible research is to use <a href="http://r-project.org">R code</a> directly inside your LaTeX document. In order to faciliate the combination of statistical code and manuscript writing, two R packages in particular have arisen:  <a href="">Sweave</a> and <a href="">knitr</a>. knitr is an R package designed as a replacement for Sweave, but both packages combine your R analysis with your <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a> manuscript (i.e., knitr = R + LaTeX).</p>

<p>One advantage of knitr is that the researcher can easily create ANOVA and demographic tables directly from the data without messing around in Excel.  However, as we'll see, both knitr and Sweave can run into problems when formatting your table values to 2 decimal points.  In this post, I'll detail my proposed method of fixing that which can be applied to your entire mansucript by editing the beginning of your knitr preamble.</p>

<!-- more -->


<p>The basic example below contains the beginning of a hypothetical Methods section of a manuscript. We want to take the values from an R table, which has the breakdown of participants by gender and ethnicity, and display them as numbers in our manuscript.</p>

<p>```tex Basic knitr.Rnw Example
\documentclass[12pt]{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}</p>

<p>&lt;&lt;setup, include=FALSE, cache=FALSE>>=
library(knitr)</p>

<h1>set global chunk options</h1>

<p>opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=90)
@</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>&lt;&lt;random-ethnicity, include=FALSE>>=</p>

<h1>Create data.frame of random ethnicities</h1>

<p>x &lt;- data.frame(Ethnicity = sample(as.factor(
  rep(x = c('White','African American','Asian American', 'Latino', 'Pacific Islander')</p>

<pre><code>)
</code></pre>

<p>  ),size = 100, replace = TRUE),
  Gender=rep(c('Male', 'Female'),100))
x.table &lt;- table(x)
@</p>

<p>\section{Methods}
We recruited \Sexpr{sum(x.table)} university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>&lt;&lt;ethnicity-table, echo = FALSE, results = 'asis'>>=
library(xtable)
xtable(x.table, caption = 'Participant Ethnicities', label='tab:ethnicity')
@</p>

<p>\end{document}
```</p>

<p>As we see below, running the <code>knit()</code> command on our knitr manuscript inside R produces a regular LaTeX file that can be compiled with to a PDF using pdflatex or <a href="http://pages.uoregon.edu/koch/texshop/">TeX Shop</a>. <em>Notice that the R table objects have been replaced with LaTeX tables.</em></p>

<p><code>r Running knit() knitr.Rnw inside R
library(knitr)
knit(input = 'knitr.Rnw', output = 'knitr.tex')
</code></p>

<p>```tex Resulting knitr.tex LaTeX document
\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
\makeatletter
\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}</p>

<p>\usepackage{framed}</p>

<p>\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX</p>

<p>\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>\section{Methods}
We recruited 200 university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{rrr}
\hline
 &amp; Female &amp; Male \
\hline
African American &amp;  22 &amp;  16 \
Asian American &amp;  26 &amp;  30 \
Latino &amp;  14 &amp;  30 \
Pacific Islander &amp;  18 &amp;  14 \
White &amp;  20 &amp;  10 \
\hline
\end{tabular}
\caption{Participant Ethnicities}
\label{tab:ethnicity}
\end{table}
\end{document}
```</p>

<p>Last, after compiling our LaTeX file using TeX Shop, we're greeted with the final product below:</p>

<p><img src="/images/knitr-example.png"></p>

<h3>Summary thus far</h3>

<p>The example above used data from R directly in a sentence in the Methods section (i.e., "We recruited 200 university undergraduates from an introductory psychology class.") and did so using the <code>\Sexpr{}</code> command in the <a href="https://en.wikipedia.org/wiki/LaTeX">knitr</a> manuscript (i.e., knitr.Rnw).  The <code>\Sexpr{}</code> command contained an <code>R</code> expression to calculate the total number participants.  This expression was evaluated and converted to LaTeX code when we ran the <code>knit()</code> function on the .Rnw file, which produces a .tex document. The .tex document contained no R code and was therefore ready to be compiled to a PDF using TeX Shop or pdf2latex in Terminal.app.</p>

<h2>Forcing knitr to round to 2 decimal places</h2>

<p>The default behavior of knitr works well <em>most</em> of the time. However, what if we didn't have whole numbers in our data table?  What if we had percentages that we wanted to round down to 2 digits, as required by many journals?  For example, the value <code>\Sexpr{pi}</code> would be evaluated and replaced with 3.141593 in the LaTeX file.  One common problem, and part of <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">Yihui's motivation</a> for replacing Sweave with <a href="http://yihui.name/knitr/"><code>knitr</code></a>, is that <code>\Sexpr{}</code> doesn't automatically round digits.</p>

<p>In Sweave (i.e., knitr's predecessor), <em>each</em> value of pi would have to be encased in <code>round(pi,2)</code>.  Thus, we end up with <code>\Sexpr{round(pi,2)}</code>.  Yihui fixed this problem by automatically rounding digits, the length of which is set with <code>options(digits=2)</code> in the knitr preamble in your .Rnw document.  See below:</p>

<p><code>tex Typical knitr preamble
&lt;&lt;&gt;&gt;=
library(knitr)
options(digits=2)
@
</code></p>

<p>The default rounding behavior of knitr works well <em>until</em> a value contains a 0 after rounding, such as 123.10.  Running the expression <code>round(123.10,2)</code> outputs 123.1. In this case, every other value in the manuscript table would be aligned at the decimal place <em>except</em> for the unlucky value - sticking out like a sore thumb. To fix this, you <em>could</em> use <code>sprintf("%.2f", pi)</code> every time you have to call <code>\Sexpr{}</code> in the manuscript - but then what's the advantage of using <a href="http://yihui.name/knitr/">knitr</a>? This hack unnecessarily complicates the manuscript and distracts from the writing process.</p>

<h3>Modify the default inline_hook for knitr</h3>

<p>After seeing a <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">StackOverflow answer by Josh O'Brien</a>, I realized that the default inline_hook function for knitr could be easily modified to use the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a> command instead of <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html"><code>round()</code></a>.  The minute change will forcibly output all manuscript values to 2 decimal places. Below, we see the default behavior for knitr when processing inline R expressions:</p>

<p><code>r knitr's Default Hook
library(knitr)
knit_hooks$get("inline")
</code></p>

<p>```
function (x)
{</p>

<pre><code>if (is.numeric(x)) 
    x = round(x, getOption("digits"))
paste(as.character(x), collapse = ", ")
</code></pre>

<p>}
```</p>

<p><em>Note:</em> My original code for this post used the <a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/format.html"><code>format()</code></a> command.  <a href="https://github.com/wch">Winston Chang</a> pointed out that this could lead to unreliable output and tweaked the code to use <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a>. The credit for the more efficient function below goes to him.  Below, we add out improved <code>inline_hook</code> to the preamble of our knitr document:</p>

<p>```tex Improving the inline_hook
&lt;&lt;>>=
library(knitr)
inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}
knit_hooks$set(inline = inline_hook)
@
```</p>

<h3>Working Example</h3>

<p>Let's put it all together!  The following is a working example of the the suggested knitr inline_hook function, which <em>should</em> give more reliable output by rounding inline values to 2 decimal places.</p>

<p>```tex Winston's Example
\documentclass[12pt]{article}
\begin{document}</p>

<p>&lt;&lt;load, include=FALSE, echo=FALSE>>=
library(knitr)</p>

<p>inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}</p>

<p>knit_hooks$set(inline = inline_hook)
@</p>

<p>Inline code looks like \Sexpr{123}, \Sexpr{123.4}, \Sexpr{123.45}, \Sexpr{123.456}.</p>

<p>And with vectors: \Sexpr{c(123, 123.4, 123.45, 123.456)}.</p>

<p>Regular output is not affected by the inline hook:</p>

<p>&lt;&lt;>>=
123
123.4
123.45
123.456</p>

<p>c(123, 123.4, 123.45, 123.456)</p>

<p>getOption('digits')
@
\end{document}
```</p>

<p><img src="/images/example.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faster Tetrachoric Correlations]]></title>
    <link href="http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations/"/>
    <updated>2013-11-07T16:48:00-08:00</updated>
    <id>http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations</id>
    <content type="html"><![CDATA[<h2>What are tetra- and polychoric correlations?</h2>

<p>Polychoric correlations estimate the correlation between two theorized normal distributions given two ordinal variables.  In psychological research, much of our data fits this definition.  For example, many survey studies used with introductory psychology pools use Likert scale items.  The responses to these items typically range from 1 (Strongly disagree) to 6 (Strongly agree).  However, we don't <em>really</em> think that person's relationship to the item is actually polytomous.  Instead, it's an imperfect approximation.</p>

<p>Similarly, tetrachorics are special cases of polychoric crrelations when the variable of interest is dichotomous. The participant may have gotten the item either correct (i.e., 1) or incorrect (i.e., 0), but the underlying knowledge that led to the items' response is probably a continuous distribution.</p>

<p>When you have polytomous rating scales but want to disattenuate the correlations to more accurately estimate the correlation betwen the latent continuous variables, one way of doing this is to use a tetrachoric or polychoric correlation coefficient.</p>

<h2>The problem</h2>

<p>At the <a href="https://sapa-project.org">SAPA Project</a>, the majority of our data is polytomous.  We ask you the degree to which you like to go to lively parties to estimate your score on latent <em>extraversion</em>.  Presently, we use <code>mixed.cor()</code>, which calls a combination of the <code>tetrachoric()</code> and <code>polychoric()</code> functions in the <code>psych</code> package (Revelle, W., 2013).</p>

<p>However, each time we build a new dataset from the website's SQL server, it takes <em>hours</em>.  And that's <strong>if</strong> everything goes well.  If there's an error in the code or a bug in a new function, it may take hours to hit the error, wasting your day.</p>

<p>After a bit of profiling, it was revealed that much of our time building the SAPA dataset was used estimating the tetrachoric and polychoric correlation coefficients.  When you do this for 250,000+ participants for 10,000+ variables, it takes <em>a long time</em>.  So Bill and I thought about how we could speed them up and feel others may benefit from our optimization.</p>

<p>A serious speedup to tetrachoric and polychoric was initiated with the help of Bill Revelle. The increase in speed is roughly 1- (nc-1)<sup>2</sup> / nc<sup>2</sup> where nc is the number of categories. Thus, for tetrachorics where nc=2, this is a 75% reduction, whereas for polychorics of 6 item responses this is just a 30% reduction.</p>

<!-- more -->


<h3>Optimizing the existing function</h3>

<p>If we call <code>psych:::tetraBinBvn</code>, we see that most of the time spent computer the tetrachorics is using <code>pmvnorm</code> to estimate each of the 4 probabilities of the tetrachoric:</p>

<p><img src="/images/tetra.png"></p>

<p>The function says for each row <em>i</em> and each column <em>j</em>, optimize a probability based on the lower and upper bounds of the row cuts and column cuts.  <strong>But wait!</strong>  Since the sum of the probabilities in the tetrachoric 2x2 matrix add to 1, why not just use a bit of subtraction to figure it out?  That would avoid waiting on <code>pvnorm</code> to compute each cell.</p>

<p>```r Old psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
for (i in 1:2) {
    for (j in 1:2) {
        P[i, j] &lt;- pmvnorm(lower = c(row.cuts[i], col.cuts[j]), 
            upper = c(row.cuts[i + 1], col.cuts[j + 1]), 
            corr = R)
    }
}
P
</code></pre>

<p>}
```</p>

<p>```r New psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
P[1, 1] &lt;- pmvnorm(lower = c(row.cuts[1], col.cuts[1]), upper = c(row.cuts[2], 
    col.cuts[2]), corr = R)
P[1, 2] &lt;- pnorm(rc) - P[1, 1]
P[2, 1] &lt;- pnorm(cc) - P[1, 1]
P[2, 2] &lt;- 1 - pnorm(rc) - P[2, 1]
P
</code></pre>

<p>}
&lt;environment: namespace:psych>
```</p>

<p>This solution resulted in a speed up of <code>tetrachor()</code> by a factor of 3!</p>

<p>Unfortunately, our trick doesn't work for the <code>polychor()</code> function as well, since there are more degrees of freedom. However, we can eliminate calling <code>pvnorm()</code> for 11 cells in a hypothetical 6x6 matrix as we only need to find the values for 25 and can use subtraction to speed up the estimation of the remaining 11.  This results in a less impressive speed up for <code>polychor()</code> that I nonetheless hope will benefit researchers.</p>

<h3>Writing a parallelized function</h3>

<p>While we were happy with our initial speed increase, it didn't have a noticeable impact on SAPA's build time.  We then began to explore ways to parallelize our existing code.  While we attempted to use various packages (i.e., <code>foreach</code>), we settled on using <code>mcmapply</code> in the existing core parallel library to minimize our dependencies.</p>

<p>As you can see below, this resulted in a speed increase by a factor of N, where N = # of cpus you allocate.  Be warned, however, that memory consumption increases dramatically as well.</p>

<p><img src="/images/polyspeed.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyze Student Exam Items using IRT]]></title>
    <link href="http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt/"/>
    <updated>2013-10-25T14:26:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt</id>
    <content type="html"><![CDATA[<p>I've cross-posted this at the <a href="http://sapa-project.org/blog/">SAPA Project's blog</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Item_response_theory">Item Response Theory</a> can be used to evaluate the effectiveness of
exams given to students.  One distinguishing feature from other paradigms is that it does not assume that every question
is equally difficult (or that the difficulty is tied to what the researcher said).  In this way, it is an empirical investigation
into the effectiveness of a given exam and can help the researcher 1) eliminate bad or problematic items and 2) judge whether the test was too difficult or the students simply didn't study.</p>

<p>In the following tutorial, we'll use <a href="http://www.r-project.org/">R</a> (R Core Team, 2013) along with the <a href="http://cran.r-project.org/web/packages/psych/index.html"><code>psych</code></a> package (Revelle, W., 2013) to look at a hypothetical exam.</p>

<!-- more -->


<p>Before we get started, remember that <code>R</code> is a programming language.  In the examples below, I perform operations on data using <a href="https://en.wikipedia.org/wiki/Function_%28computer_science%29"><em>functions</em></a> like <a href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.html"><code>cor</code></a> and <a href="http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.csv</code></a>.  We can also save the output as <a href="https://en.wikipedia.org/wiki/Object_%28computer_science%29"><em>objects</em></a> using the assignment arrow, <code>&lt;-</code>. It's a bit different from a point-and-click program like SPSS, but you <em>don't</em> need to know how to program to analyze exams and questions using IRT!</p>

<p>First, load the the <code>psych</code> package.  Next, load the students' grades into R using <code>read.csv()</code> from the <code>psych</code> package.</p>

<p>```r</p>

<h1>Load the psych package</h1>

<p>library(psych)</p>

<h1>Read the csv file and save it as grades</h1>

<p>grades &lt;- read.csv(file = "~/Downloads/Grades.csv")
```</p>

<p>Notice that we are using item-level grades, where each row is a given student and each cell is the number of points received on that question.  In my example, V1, V2, etc. correspond to exam questions.  Your matrix or data frame should look like this:</p>

<p><code>r
head(grades)
</code></p>

<p>```</p>

<h2>V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20</h2>

<h2>1  2  2  2  2  4  2  3  2  4   2   2   3   5   3   6   2   4   4   4   3</h2>

<h2>2  2  0  2  2  0  0  3  0  4   2   0   1   3   0   0   2   2   1   0   0</h2>

<h2>3  2  2  2  2  0  2  3  2  4   0   0   3   5   0   5   2   4   2   4   2</h2>

<h2>4  2  1  2  2  3  0  0  2  4   2   2   3   1   1   2   0   4   1   4   3</h2>

<h2>5  2  2  2  2  4  2  3  2  4   2   2   3   5   3   4   2   4   4   4   3</h2>

<h2>6  1  0  2  2  0  2  0  2  0   2   1   2   5   0   3   2   4   2   4   3</h2>

<h2>V21 V22 V23 V24 V25 V26 V27 Total</h2>

<h2>1   2   2   4   4   6   6   6    91</h2>

<h2>2   0   0   0   3   3   6   6    42</h2>

<h2>3   2   1   2   4   6   5   6    72</h2>

<h2>4   0   2   4   4   0   0   0    49</h2>

<h2>5   2   2   4   4   5   6   6    88</h2>

<h2>6   0   0   4   4   6   4   3    58</h2>

<p>```</p>

<p>Next, compute the <a href="https://en.wikipedia.org/wiki/Polychoric_correlation">polychoric correlations</a> on the raw grades (not including the Total column).  By using polychoric correlations, we estimate the normal distribution of latent content knowledge, which can be underestimated if Pearson correlations are instead used on polytomous items (see  Lee, Poon &amp; Bentler, 1995).</p>

<p>```r</p>

<h1>Perform a polychoric correlation on grades and save it as grades.poly</h1>

<p>grades.poly &lt;- polychoric(x = grades[, 1:27], polycor = TRUE)
```</p>

<p>Now that we have the polychoric correlations, we can run <code>irt.fa()</code> on the dataset to see the item difficulties and information.</p>

<p>```r</p>

<h1>Using grades.poly, perform an irt and save it as grades.irt</h1>

<p>grades.irt &lt;- irt.fa(x = grades.poly, plot = TRUE)</p>

<h1>Plot the output from grades.irt</h1>

<p>plot(grades.irt)
```</p>

<p><img src="/images/grades.png"></p>

<p>Thus, we have some great items that have a lot of information about students of average and low content knowledge (e.g., V24, V17, V18), but not enough to distinguish the high-knowledge students.  In redesigning an exam for next semester or year, we might save the best performing questions while trying to rewrite the existing questions or trying new questions.</p>

<p>Next, let's see what how well the test did <em>overall</em> at distinguishing students:</p>

<p><code>r
plot.irt(type = "test", grades.irt)
</code></p>

<p><img src="/images/test.png"></p>

<p>The second plot shows the <em>test performance</em>.  We have great reliability for distinguishing who didn't study (lowers end of our latent trait), but overall the test may have been too easy (opposite of my prediction).  It's important that the test is not too difficult to discourage students, but the graph above suggests that we had very low information at how students that studied were different from eachother.  This is again reflected by the histogram plotted in the next section, where high scoring students seem to cluster together.</p>

<h2>Rescaling the test</h2>

<p>While many students in our hypothetical dataset did very well on the exam, instructors may
need to rescale their exam so that the mean grade is an 85% or 87.5%.  Using the <code>scale()</code> function (see also <a href="http://personality-project.org/r/psych/help/rescale.html"><code>rescale()</code></a> in <code>psych</code> package), we
can ensure that the <a href="https://en.wikipedia.org/wiki/Ranking#Ranking_in_statistics?s">rank-order</a> distribution of the students is preserved (allowing us to distinguish
those who studied well from those who didn't), while scaling the sample distribution to fit in with other classes in your department.</p>

<p>Currently, our scores are in cumulative raw points.  Notice that we divide the Total points column by 91 to convert the histogram into grade percentages.  Let's plot a histogram to see the distribution of scores.</p>

<p>```r</p>

<h1>Load the ggplot2 library</h1>

<p>library(ggplot2)</p>

<h1>Plot a histogram</h1>

<p>qplot((Total/91)*100, data=grades, geom="histogram",xlab='Raw Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/raw.png"></p>

<p>The distribution has a mean 71.68 percent and a standard deviation of 15.54.  Given grade inflation,
it may look like your students are doing poorly when in fact the distribution is similiar to other courses
being taught.  Next, we can rescale the grades, creating a mean of 87.5 and a standard deviation of 7.5.  These numbers are arbitrary so use your best judgement.</p>

<p>```r
scaled.grades &lt;- scale(grades$Total) * 7.5 + 87.5
qplot(scaled.grades, xlab='Scaled Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/scaled.png"></p>

<p>The second distribution may be preferred, depending on your needs.  With the raw distribution, we would have had 45% of the students receiving grades below a C-, assuming a normal distribution (for the curious, R can calculate these probabilites using the <code>pnorm</code> function: <code>pnorm(q=70,mean=71.68,sd=15.54)</code>).  Now, 0.9% of students would fall below the 70% cutoff.  Again, my mean and standard deviation chosen in the above example are arbitrary.</p>

<h2>References</h2>

<ul>
<li><p>Lee, S. Y., Poon, W. Y., &amp; Bentler, P. M. (1995). <em>A two‚Äêstage estimation of structural equation models with continuous and polytomous variables</em>. British Journal of Mathematical and Statistical Psychology, 48(2), 339-358.</p></li>
<li><p>R Core Team (2013). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing, Vienna, Austria. <a href="http://www.R-project.org/">http://www.R-project.org/</a>.</p></li>
<li><p>Revelle, W. (2013). <em>psych: Procedures for Personality and Psychological Research</em>. Northwestern University, Evanston, Illinois, USA. <a href="http://CRAN.R-project.org/package=psych">http://CRAN.R-project.org/package=psych</a>. Version = 1.3.10.</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Easy Sweave for LaTeX and R]]></title>
    <link href="http://frenchja.github.com/blog/2013/08/16/easy-sweaving-for-latex-and-r/"/>
    <updated>2013-08-16T12:41:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2013/08/16/easy-sweaving-for-latex-and-r</id>
    <content type="html"><![CDATA[<p>When you're writing up reports using statistics from R, it can be tiresome
to constantly copy and paste results from the R Console.  To get around this, many of us use Sweave, which allows us to <em>embed</em> R code in LaTeX files.
<a href="https://en.wikipedia.org/wiki/Sweave">Sweave</a> is an R function that converts R code to LaTeX, a document typesetting language.  This enables accurate, shareable analyses as well as high-resolution graphs that are publication quality.</p>

<p>Needless to say, the marriage of statistics with documents makes writing up APA-style reports a bit easier, especially with Brian Beitzel's amazing <a href="http://www.ctan.org/pkg/apa6"><code>apa6</code> class for LaTeX</a>.</p>

<!-- more -->


<h2>Making Sweave Available Systemwide</h2>

<p>However, Sweave doesn't always work correctly.  One common complaint that you'll get after Sweaving a file is <code>Sweave.sty not found!</code>. While Sweave.sty is a LaTeX package, it doesn't <em>live</em> with the rest of the LaTeX packages because it's installed using R.  Many people try to solve this by copying and pasting Sweave.sty into every document directory, but I'm sharing a better way below.</p>

<p>Using <a href="https://en.wikipedia.org/wiki/Terminal_%28OS_X%29">Terminal.app</a>:
<code>bash Go to the MacTeX or TeX Live local directory.
cd /usr/local/texlive/texmf-local/tex/latex
</code></p>

<p><code>bash Form a link between the R Sweave.sty files and MacTeX
sudo ln -s /Library/Frameworks/R.framework/Resources/share/texmf/ Sweave
</code></p>

<p><code>bash Tell MacTeX/TeX Live to recognize the file and rebuild the database
sudo mktexlsr
</code></p>

<p>If using <code>mktexlsr</code> results in a <code>command not found</code> error, the TeX Live distribution probably isn't in your <a href="https://en.wikipedia.org/wiki/PATH_%28variable%29">$PATH</a>, but you can hunt for the program anyway.  For example, if you're using MacTeX 2013, the program will be found in a directory similar to this:</p>

<p><code>bash
sudo /usr/local/texlive/2013/bin/x86_64-darwin/mktexlsr
</code></p>

<h2>Using Sweave in TeXShop</h2>

<p>When you're getting started with LaTeX, many Mac users prefer the bundled
editor, TeXShop.  <a href="http://cameron.bracken.bz/sweave-for-texshop">Cameron Bracken</a> gives us a helpful piece of code that allows easy Sweaving straight from TeXShop.  TeXShop uses various <em>engines</em> that allow it to render LaTeX.  Using a bit of <a href="https://en.wikipedia.org/wiki/Bash_%28Unix_shell%29"><code>BASH</code></a> scripting, we can write our own Sweave engine and make it available right within TeXShop.  I have adapted Cameron's original engine to accomodate Bibtex citations (<a href="https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#Why_won.27t_LaTeX_generate_any_output.3F">see here</a>).</p>

<p>Using a text editor, paste in the following syntax and save the file as Sweave.engine:</p>

<p>```bash Sweave.engine</p>

<h1>!/bin/env bash</h1>

<p>export PATH=$PATH:/usr/texbin:/usr/local/bin
R CMD Sweave "$1"
pdflatex "${1%.<em>}"
bibtex "${1%.</em>}.aux"
pdflatex "${1%.*}"</p>

<h1>If you run pdflatex again you get citations</h1>

<p>pdflatex "${1%.*}"
```</p>

<p>Next, in Terminal.app, move the file to the TeXShop engines folder:</p>

<p><code>bash Enable the Sweave.engine
mv Sweave.engine ~/Library/TeXShop/Engines/
chmod +x ~/Library/TeXShop/Engines/Sweave.engine
</code></p>

<p><img src="/images/sweaveengine.png"></p>

<p>Now restart TeXShop if it's running and you should see Sweave as an available option!</p>

<p><img src="/images/sweave.png"></p>
]]></content>
  </entry>
  
</feed>
