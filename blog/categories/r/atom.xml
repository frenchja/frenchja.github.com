<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: R | Jason A. French]]></title>
  <link href="http://frenchja.github.com/blog/categories/r/atom.xml" rel="self"/>
  <link href="http://frenchja.github.com/"/>
  <updated>2014-07-26T18:14:43-07:00</updated>
  <id>http://frenchja.github.com/</id>
  <author>
    <name><![CDATA[Jason A. French]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recursion in R]]></title>
    <link href="http://frenchja.github.com/blog/2014/07/26/recursion-in-r/"/>
    <updated>2014-07-26T15:49:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2014/07/26/recursion-in-r</id>
    <content type="html"><![CDATA[<p>Most technical interviews with companies will ask
you to whiteboard code some type of recursive function
in your favorite programming language.  Although Python
seems to be the dominate king in data science, recursion
can be a powerful tool in R.</p>

<h2>What is recursion?</h2>

<p>Recursive functions call <em>themselves</em>.  That is, they
break down the problem into the smallest possible
components and the <code>function()</code> calls itself within the
original function() on each of the smaller components.
Afterward, the results are put together to solve the
original problem.  Let's take a look at more concrete examples.</p>

<!-- more -->


<h2>Quicksort</h2>

<p>A technical interview will usually ask you to implement
some type of sorting function that can be solved using
a recursive algorithm.  Let's try implementing <a href="https://en.wikipedia.org/wiki/Quicksort">quicksort</a> in R:</p>

<p>```r Programming Quicksort in R</p>

<h1>!/usr/bin/env Rscript</h1>

<h1>Author:  Jason A. French</h1>

<p>quickSort &lt;- function(vect) {</p>

<pre><code># Args:
#   vect: Numeric Vector

# Stop if vector has length of 1
if (length(vect) &lt;= 1) {
    return(vect)
}
# Pick an element from the vector
element &lt;- vect[1]
partition &lt;- vect[-1]
# Reorder vector so that integers less than element
# come before, and all integers greater come after.
v1 &lt;- partition[partition &lt; element]
v2 &lt;- partition[partition &gt;= element]
# Recursively apply steps to smaller vectors.
v1 &lt;- quickSort(v1)
v2 &lt;- quickSort(v2)
return(c(v1, element, v2))
</code></pre>

<p>}
```</p>

<p><code>r
quickSort(c(4, 65, 2, -31, 0, 99, 83, 782, 1))
[1] -31   0   1   2   4  65  83  99 782
</code></p>

<h2>Merge Sort</h2>

<p>A second sorting algorithm that we can implement using recursion is the <a href="https://en.wikipedia.org/wiki/Merge_sort">Merge Sort</a>.
Sorting algorithms are important because they differ in their <em>speed</em>, depending on the
nature of the input data.  In the case of merge sort, the <em>worst possible outcome</em> for
the time it takes to sort the data is <em>n * log(n)</em>, where <em>n</em> is the length of your list of
numbers.</p>

<p>Like Quicksort, we're splitting the vector into smaller sub-vectors by halving it until
each vector has just one integer.  We then merge the vectors back together, this time in order. Let's tackle this algorithm in R as well.</p>

<p>I found that <a href="http://rosettacode.org/">Rosetta Code</a> already provides a good example, so below is the their public domain code with my
comments added to the code for explanation.</p>

<p>```r Merge sort in R</p>

<h1>!/usr/bin/env Rscript</h1>

<h1>http://rosettacode.org/wiki/Sorting_algorithms/Merge_sort#R</h1>

<p>mergesort &lt;- function(m)</p>

<p>{
   merge_ &lt;- function(left, right)
   # Recursive function to compare and append values in order
   {</p>

<pre><code>  # Create a list to hold the results
  result &lt;- c()
  # This is our stop condition. While left and right contain 
  # a value, compare them
  while(length(left) &gt; 0 &amp;&amp; length(right) &gt; 0)
  {
     # If left is less than or equal to right,
     # add it to the result list
     if(left[1] &lt;= right[1])
     {
        result &lt;- c(result, left[1])
        # Remove the value from the list
        left &lt;- left[-1]
     } else
     {
        # When right is less than or equal to left,
        # add it to the result.
        result &lt;- c(result, right[1])
        # Remove the appended integer from the list.
        right &lt;- right[-1]
     }         
  }
  # Keep appending the values to the result while left and right
  # exist.
  if(length(left) &gt; 0) result &lt;- c(result, left)
  if(length(right) &gt; 0) result &lt;- c(result, right)
  result
</code></pre>

<p>   }</p>

<p>   # Below is our stop condition for the mergesort function.
   # When the length of the vector is 1, just return the integer.
   len &lt;- length(m)
   if(len &lt;= 1) m else
   {</p>

<pre><code>  # Otherwise keep dividing the vector into two halves.
  middle &lt;- length(m) / 2
  # Add every integer from 1 to the middle to the left
  left &lt;- m[1:floor(middle)]
  right &lt;- m[floor(middle+1):len]
  # Recursively call mergesort() on the left and right halves.
  left &lt;- mergesort(left)
  right &lt;- mergesort(right)
  # Order and combine the results.
  if(left[length(left)] &lt;= right[1])
  {
     c(left, right)
  } else
  {
     merge_(left, right)
  } 
</code></pre>

<p>   }
}
```</p>

<!--
## Binary Search Tree
Another question commonly asked in tech interviews is to 
implement a [binary search tree](https://en.wikipedia.org/wiki/Binary_search_tree).  Once again, these types of questions are common with Java or Python but R programmers can implement them 
just as well. A binary search tree is a node-based way to organize data.  The data structure generally follows some law, such as "each value in the left child is less than the parent node, and each value in the right child is greater than the parent node."

<img src="images/binarytree.png" title="Example Binary Tree" >

In the picture above, 7 is in the *root* of the tree.  All other 
values follow my rule above.

While implementing a BST is non-trivial in Python, it becomes 
more complex in R as R doesn't have pointer [variables](https://en.wikipedia.org/wiki/Pointer_%28computer_programming%29) as far as I know.  However, we can still implement our data structure as a series of matrices, where 
each row contains the c(left, right, root) for each node.

```r
print.tree <- function(index, tree) {
    # Args:
    #   index: Tree row
    #   tree
    left <- tree$matrix[index, 1]
    print(tree$matrix[index, 3]
}

new.tree <- function(value, nrow) {
    m <- matrix(rep(NA, nrow*3, ncol = 3)
    # Place the first value in the 3rd root column
    m[1, 3] <- value
    return(list(mat = m, next = 2, nrow = nrow))
}

insert.tree <- function(index, tree, new.value) {
    # Inserts a new value into the sub-tree, with the root being
    # at 'index'.

    # Should the value go to the left or right child?
    direction <- if(new.value <= tree$matrix[index, 3]){
            1} else {
            2}
    # If that child is NA, place new.value.  Otherwise recurse down.
    if(is.na(tree$matrix[index, direction])) {
        new.index <- tree$next
        
        if(tree$next == nrow(tree$matrix) + 1) {


}


```
-->


<h3>References</h3>

<ul>
<li>Matloff, N., &amp; Matloff, N. S. (2011). The art of R programming: a tour of statistical software design. No Starch Press.</li>
<li>https://stackoverflow.com/questions/8797089/how-to-walk-up-a-tree-in-r</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using R with MySQL Databases]]></title>
    <link href="http://frenchja.github.com/blog/2014/07/03/using-r-with-mysql-databases/"/>
    <updated>2014-07-03T10:58:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2014/07/03/using-r-with-mysql-databases</id>
    <content type="html"><![CDATA[<h2>Overview</h2>

<p>When I began using R, like most researchers I kept all my data in some combination
of R's native <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html">data.frame</a>
format or a <a href="http://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> file that my analysis would continually read.
However, as I began to analyze big datasets at the <a href="https://sapa-project.org">SAPA Project</a>
and at <a href="http://insightdatascience.com/">Insight</a>,
I realized that there is a lot of value to instead keeping your data in a MySQL database
and streaming it into R when necessary.
This post will briefly outline a few advantages of using a database to store data and run through a
basic example of using R to transfer data to MySQL.</p>

<!-- more -->


<h3>Memory Efficiency</h3>

<p>Relational databases, such as MySQL, organize data into <em>tables</em> (like a spreadsheet)
and can link values in tables to each other.  Generally speaking, they are better at handling large datasets and are more efficient
at storing and accessing data than CSVs due to compression and indexing.
The data stay in the MySQL database until accessed via a query, which is
different than how R approaches data.frames and CSVs.
When accessing data stored in a data.frame or CSV file in R, the data must all fit <em>in memory</em>.
However, this becomes a problem if when using a large dataset or if you're cursed with an older computer with &lt;= 4GB of RAM.
In these cases, every time you load your dataset or do a memory-intensive operation (e.g., polychoric correlations)
your computer will slow to a painful crawl as your hard drives grind while your operating system
switches to using <a href="http://en.wikipedia.org/wiki/Virtual_memory">virtual memory</a> or swap space, rather than RAM.
You could get around this by using random sampling to create a smaller subset (or multiple subsets
if you want to bootstrap), but you can also use this technique on a SQL database. The advantage is that SQL databases
only load the data you're working with into your local machine's RAM when you SELECT the data you need - leaving plenty of
memory for the actual analysis.</p>

<h3>Safety and Security</h3>

<p>Two additional reasons I prefer SQL databases are safety and security. Services such as
Amazon's RDS (i.e., Relational Database Service) offer frequent backups and easy replication
across instances. If the local machine dies, the data are safe and uncorrupted. If I were storing
data in an .Rdata file, I'd have to rely on my OS (e.g., Time Machine) or a cloud-based storage solution
(e.g., Dropbox or CrashPlan) to backup my data. Worse, if I were storing my data in an Excel database and Excel
crashed, I may risk losing years of progress.</p>

<p>Using MySQL, I can give a collaborator access
to only needed portions of the data by locking down their SQL user to certain tables
and operations. By limiting access, we limit risk of corruption and overwriting years of progress.</p>

<h3>Scalable as the need grows</h3>

<p>Last, I find databases such as MySQL to be more scalable due to cloud computing. In the case of using Amazon's RDS,
you can buy as much storage or bandwidth as you need to complete your analysis, dissertation,
or build your web app. If I need multiple nodes or machines to analyze the data, I don't need to keep a copy of
the data on each machine.  I simply pass the SQL authentication parameters to each machine and the
data is accessed from a central location.</p>

<p>So let's begin by setting up a <a href="http://aws.amazon.com/rds/free/">Free-Tier RDS service</a> on Amazon and then move some
data from R to the RDS database.</p>

<h2>Setting up an RDS Instance</h2>

<p>Amazon currently offers a Free Tier for their RDS storage.  It offers:</p>

<ul>
<li>Enough hours to run a DB Instance continuously each month,</li>
<li>20 GB of database storage,</li>
<li>10 million I/O operations, and</li>
<li>automated database backups.</li>
</ul>


<p>To setup an RDS instance:</p>

<ol>
<li>Sign into the AWS Management Console.</li>
<li>Select 'RDS' under the 'Database' group.</li>
<li>Click 'Launch DB Instance'.</li>
<li>Select 'MySQL' for the database engine.</li>
<li>Select 'No' when asked if you intend to use the database for production purposes. Accidentally selecting 'Yes' could result in hefty fees.</li>
<li>For 'Instance Class', select 'db.t1.micro', 'No' for 'Multi-AZ Deployment', and 20 GB for 'Allocated Storage'.</li>
<li>Click 'Next' and launch your RDS instance.</li>
</ol>


<p>At this point, you should have a blank RDS database with a master username and password combination. Be sure to test that you can connect - I suggest using <a href="http://www.sequelpro.com/">Sequel Pro</a>. <em>If you can't connect,
you may need to open up port 3306 in your EC2 Security Group</em>.</p>

<p>Click on your new RDS Instance to get the hostname to connect to and note the database name:</p>

<p><img class="center" src="/images/rds1.png"></p>

<p>Fire up Sequel Pro and test out your new database.  If you want to connect using SSL, you'll need the SSH key you generated when
you first created your EC2 instance (not covered here).</p>

<p><img class="center" src="/images/rds2.png"></p>

<h2>Installing the RMySQL package</h2>

<p>Previously, I found installing <a href="http://cran.r-project.org/web/packages/RMySQL/index.html">RMySQL</a> to be very difficult when I was using MacPorts.  However,
having switched to <a href="http://brew.sh/">Homebrew</a> on my new machine, RMySQL found all the MySQL headers and libraries as
<a href="http://brew.sh/">Homebrew</a> uses the <code>/usr/local</code> directory to store your software, which is already in your shell $PATH.</p>

<p>Before compiling RMySQL, first compile MySQL using Homebrew:</p>

<p>```bash Installing MySQL using Homebrew</p>

<h1>If you haven't already, install Homebrew:</h1>

<h1>ruby -e "$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)"</h1>

<h1>Install MySQL</h1>

<p>brew install mysql
```</p>

<p>```
==> Downloading https://downloads.sf.net/project/machomebrew/Bottles/mysql-5.6.1
Already downloaded: /Library/Caches/Homebrew/mysql-5.6.19.mavericks.bottle.tar.gz
==> Pouring mysql-5.6.19.mavericks.bottle.tar.gz
==> Caveats
A "/etc/my.cnf" from another install may interfere with a Homebrew-built
server starting up correctly.</p>

<p>To connect:</p>

<pre><code>mysql -uroot
</code></pre>

<p>To have launchd start mysql at login:</p>

<pre><code>ln -sfv /usr/local/opt/mysql/*.plist ~/Library/LaunchAgents
</code></pre>

<p>Then to load mysql now:</p>

<pre><code>launchctl load ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist
</code></pre>

<p>Or, if you don't want/need launchctl, you can just run:</p>

<pre><code>mysql.server start
</code></pre>

<p>==> Summary
🍺  /usr/local/Cellar/mysql/5.6.19: 9536 files, 338M</p>

<p>```</p>

<p>Next, compile RMySQL from source:</p>

<p><code>r Compiling RMySQL
install.packages('RMySQL', type = 'source')
</code></p>

<p>```
trying URL 'http://cran.rstudio.com/src/contrib/RMySQL_0.9-3.tar.gz'
Content type 'application/x-gzip' length 165363 bytes (161 Kb)</p>

<h1>opened URL</h1>

<p>downloaded 161 Kb</p>

<ul>
<li>installing <em>source</em> package ‘RMySQL’ ...
<strong> package ‘RMySQL’ successfully unpacked and MD5 sums checked
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for compress in -lz... yes
checking for getopt_long in -lc... yes
checking for mysql_init in -lmysqlclient... yes
checking for egrep... grep -E
checking for ANSI C header files...
rm: conftest.dSYM: is a directory
rm: conftest.dSYM: is a directory
yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking mysql.h usability... no
checking mysql.h presence... no
checking for mysql.h... no
checking /usr/local/include/mysql/mysql.h usability... yes
checking /usr/local/include/mysql/mysql.h presence... yes
checking for /usr/local/include/mysql/mysql.h... yes
configure: creating ./config.status
config.status: creating src/Makevars
</strong> libs
clang -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include/mysql -I/usr/local/include -I/usr/local/include/freetype2 -I/opt/X11/include    -fPIC  -Wall -mtune=core2 -g -O2  -c RS-DBI.c -o RS-DBI.o
clang -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG -I/usr/local/include/mysql -I/usr/local/include -I/usr/local/include/freetype2 -I/opt/X11/include    -fPIC  -Wall -mtune=core2 -g -O2  -c RS-MySQL.c -o RS-MySQL.o
clang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o -lmysqlclient -lz -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation
installing to /Library/Frameworks/R.framework/Versions/3.1/Resources/library/RMySQL/libs
<strong> R
</strong> inst
<strong> preparing package for lazy loading
Creating a generic function for ‘format’ from package ‘base’ in package ‘RMySQL’
Creating a generic function for ‘print’ from package ‘base’ in package ‘RMySQL’
</strong> help
<strong>* installing help indices
</strong> building package indices
** testing if installed package can be loaded</li>
<li>DONE (RMySQL)
```</li>
</ul>


<h2>Transferring Existing Data to/from MySQL</h2>

<p>Having a database is useless unless we can easily convert our existing data.frames
into MySQL tables.  Let's try this using the <code>Thurstone</code> dataset from the Revelle's <a href="http://cran.r-project.org/web/packages/psych/index.html"><code>psych</code></a> package:</p>

<p>```r Moving Thurstone to RDS
library(RMySQL)
library(psych)
con &lt;- dbConnect(MySQL(),</p>

<pre><code>user = 'RDSUser',
password = 'YourPass',
host = 'RDS Host',
dbname='YourDB')
</code></pre>

<p>dbWriteTable(conn = con, name = 'Test', value = as.data.frame(Thurstone))
```</p>

<p>There! Now we have transferred our data.frame to our SQL database.</p>

<p><img class="center" src="/images/rds3.png"></p>

<p>Similarly, if you want to read data <em>from</em> MySQL to R, we can use the <code>dbReadTable()</code> function,
which returns a data.frame object. You can keep your data in MySQL and stream portions into R for specific analyses.
Since <code>dbReadTable</code> outputs a data.frame, we can use this command in place of a data.frame.</p>

<p>Let's run an <code>omega()</code> factor analysis on the Thurstone table:</p>

<p><code>r Thurstone Omega
omega(dbReadTable(conn = con,name = 'Test'), title = "9 variables from Thurstone")
</code></p>

<p>```
Loading required package: MASS
Loading required package: GPArotation
Loading required package: parallel
9 variables from Thurstone
Call: omega(m = dbReadTable(conn = con, name = "Test"), title = "9 variables from Thurstone")
Alpha:                 0.89
G.6:                   0.91
Omega Hierarchical:    0.74
Omega H asymptotic:    0.79
Omega Total            0.93</p>

<p>Schmid Leiman Factor loadings greater than  0.2</p>

<pre><code>               g   F1*   F2*   F3*   h2   u2   p2
</code></pre>

<p>Sentences       0.71  0.57             0.82 0.18 0.61
Vocabulary      0.73  0.55             0.84 0.16 0.63
Sent_Completion 0.68  0.52             0.73 0.27 0.63
First_Letters   0.65        0.56       0.73 0.27 0.57
X4_Letter_Words 0.62        0.49       0.63 0.37 0.61
Suffixes        0.56        0.41       0.50 0.50 0.63
Letter_Series   0.59              0.61 0.72 0.28 0.48
Pedigrees       0.58  0.23        0.34 0.50 0.50 0.66
Letter_Group    0.54              0.46 0.53 0.47 0.56</p>

<p>With eigenvalues of:
   g  F1<em>  F2</em>  F3*
3.58 0.96 0.74 0.71</p>

<p>general/max  3.71   max/min =   1.35
mean percent general =  0.6    with sd =  0.05 and cv of  0.09
Explained Common Variance of the general factor =  0.6</p>

<p>The degrees of freedom are 12  and the fit is  0.01</p>

<p>The root mean square of the residuals is  0.01
The df corrected root mean square of the residuals is  0.01</p>

<p>Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 27  and the fit is  1.48</p>

<p>The root mean square of the residuals is  0.14
The df corrected root mean square of the residuals is  0.16</p>

<p>Measures of factor score adequacy</p>

<pre><code>                                             g  F1*  F2*  F3*
</code></pre>

<p>Correlation of scores with factors            0.86 0.73 0.72 0.75
Multiple R square of scores with factors      0.74 0.54 0.52 0.56
Minimum correlation of factor score estimates 0.49 0.08 0.03 0.11</p>

<p> Total, General and Subset omega for each subset</p>

<pre><code>                                             g  F1*  F2*  F3*
</code></pre>

<p>Omega total for total scores and subscales    0.93 0.92 0.83 0.79
Omega general for total scores and subscales  0.74 0.58 0.50 0.47
Omega group for total scores and subscales    0.16 0.35 0.32 0.32
```</p>

<p><img class="center" src="/images/rds4.png"></p>

<p>In other analyses, you may only need a fraction of the MySQL table.  In these cases, we'd want to use the <code>dbGetQuery()</code> command and issue a MySQL statement to SELECT and aggregate the information.  Let's get a vector of Sentences correlations that are greater than 0.5:</p>

<p><code>r
dbGetQuery(conn = con, statement = "SELECT Sentences FROM Test WHERE Sentences &gt; 0.5;")
</code></p>

<p><code>
  Sentences
1     1.000
2     0.828
3     0.776
4     0.541
</code></p>

<h3>Safely storing credentials</h3>

<p>It's important to note that in the above example, although I passed the <code>user</code> and <code>password</code>
parameters within my <code>dbConnect()</code> function, you would <strong>never</strong> want to do this in production code that
you were sharing.  It's far better to store the credentials a text file where <code>dbConnect(MySQL())</code> will look when you don't pass any credentials:</p>

<p><code>bash ~/.my.cnf
user = youruser
password = yourpassword
host = yourhouse
dbname = yourdb
</code></p>

<h2>Useful Packages for Fast DB Operations</h2>

<p>Ideally, we want to have one set of analysis code that is agnostic to <em>how</em>
the data are stored.  I suggest looking into <a href="http://had.co.nz/">Hadley Wickham's</a> new <a href="http://blog.rstudio.org/2014/01/17/introducing-dplyr/"><code>dplyr</code></a> package
(<a href="https://github.com/hadley/dplyr">Github</a>). <code>dplyr</code> is able to filter, sort, group, and summarize data
quickly whether it is stored in a data.frame, data.table, or database. Database operations may not always be as
fast as a data.table operation, but again, the advantage is that you don't need to feed all of your data into memory.</p>

<p>Hadley provides a few vignettes for getting getting started with <code>dplyr</code>:</p>

<p><code>r
vignette("introduction", package = "dplyr")
vignette("databases", package = "dplyr")
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fixing knitr: Formatting statistical output to 2 digits in R]]></title>
    <link href="http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits/"/>
    <updated>2014-04-25T16:53:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits</id>
    <content type="html"><![CDATA[<h3>Overview of reproducible research</h3>

<p>Reproducible research is a phrase that describes an academic paper or manuscript that contains the code and data in addition to what is usually published - the researcher's interpretation.  In doing so, the experimental design and method of analysis is easily replicated by unaffiliated labs and critiqued by reviewers as the full analysis used to produce the results is submitted along with the final paper.  One way of producing reproducible research is to use <a href="http://r-project.org">R code</a> directly inside your LaTeX document. In order to faciliate the combination of statistical code and manuscript writing, two R packages in particular have arisen:  <a href="">Sweave</a> and <a href="">knitr</a>. knitr is an R package designed as a replacement for Sweave, but both packages combine your R analysis with your <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a> manuscript (i.e., knitr = R + LaTeX).</p>

<p>One advantage of knitr is that the researcher can easily create ANOVA and demographic tables directly from the data without messing around in Excel.  However, as we'll see, both knitr and Sweave can run into problems when formatting your table values to 2 decimal points.  In this post, I'll detail my proposed method of fixing that which can be applied to your entire mansucript by editing the beginning of your knitr preamble.</p>

<!-- more -->


<p>The basic example below contains the beginning of a hypothetical Methods section of a manuscript. We want to take the values from an R table, which has the breakdown of participants by gender and ethnicity, and display them as numbers in our manuscript.</p>

<p>```tex Basic knitr.Rnw Example
\documentclass[12pt]{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}</p>

<p>&lt;&lt;setup, include=FALSE, cache=FALSE>>=
library(knitr)</p>

<h1>set global chunk options</h1>

<p>opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=90)
@</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>&lt;&lt;random-ethnicity, include=FALSE>>=</p>

<h1>Create data.frame of random ethnicities</h1>

<p>x &lt;- data.frame(Ethnicity = sample(as.factor(
  rep(x = c('White','African American','Asian American', 'Latino', 'Pacific Islander')</p>

<pre><code>)
</code></pre>

<p>  ),size = 100, replace = TRUE),
  Gender=rep(c('Male', 'Female'),100))
x.table &lt;- table(x)
@</p>

<p>\section{Methods}
We recruited \Sexpr{sum(x.table)} university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>&lt;&lt;ethnicity-table, echo = FALSE, results = 'asis'>>=
library(xtable)
xtable(x.table, caption = 'Participant Ethnicities', label='tab:ethnicity')
@</p>

<p>\end{document}
```</p>

<p>As we see below, running the <code>knit()</code> command on our knitr manuscript inside R produces a regular LaTeX file that can be compiled with to a PDF using pdflatex or <a href="http://pages.uoregon.edu/koch/texshop/">TeX Shop</a>. <em>Notice that the R table objects have been replaced with LaTeX tables.</em></p>

<p><code>r Running knit() knitr.Rnw inside R
library(knitr)
knit(input = 'knitr.Rnw', output = 'knitr.tex')
</code></p>

<p>```tex Resulting knitr.tex LaTeX document
\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
\makeatletter
\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}</p>

<p>\usepackage{framed}</p>

<p>\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX</p>

<p>\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>\section{Methods}
We recruited 200 university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{rrr}
\hline
 &amp; Female &amp; Male \
\hline
African American &amp;  22 &amp;  16 \
Asian American &amp;  26 &amp;  30 \
Latino &amp;  14 &amp;  30 \
Pacific Islander &amp;  18 &amp;  14 \
White &amp;  20 &amp;  10 \
\hline
\end{tabular}
\caption{Participant Ethnicities}
\label{tab:ethnicity}
\end{table}
\end{document}
```</p>

<p>Last, after compiling our LaTeX file using TeX Shop, we're greeted with the final product below:</p>

<p><img src="/images/knitr-example.png"></p>

<h3>Summary thus far</h3>

<p>The example above used data from R directly in a sentence in the Methods section (i.e., "We recruited 200 university undergraduates from an introductory psychology class.") and did so using the <code>\Sexpr{}</code> command in the <a href="https://en.wikipedia.org/wiki/LaTeX">knitr</a> manuscript (i.e., knitr.Rnw).  The <code>\Sexpr{}</code> command contained an <code>R</code> expression to calculate the total number participants.  This expression was evaluated and converted to LaTeX code when we ran the <code>knit()</code> function on the .Rnw file, which produces a .tex document. The .tex document contained no R code and was therefore ready to be compiled to a PDF using TeX Shop or pdf2latex in Terminal.app.</p>

<h2>Forcing knitr to round to 2 decimal places</h2>

<p>The default behavior of knitr works well <em>most</em> of the time. However, what if we didn't have whole numbers in our data table?  What if we had percentages that we wanted to round down to 2 digits, as required by many journals?  For example, the value <code>\Sexpr{pi}</code> would be evaluated and replaced with 3.141593 in the LaTeX file.  One common problem, and part of <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">Yihui's motivation</a> for replacing Sweave with <a href="http://yihui.name/knitr/"><code>knitr</code></a>, is that <code>\Sexpr{}</code> doesn't automatically round digits.</p>

<p>In Sweave (i.e., knitr's predecessor), <em>each</em> value of pi would have to be encased in <code>round(pi,2)</code>.  Thus, we end up with <code>\Sexpr{round(pi,2)}</code>.  Yihui fixed this problem by automatically rounding digits, the length of which is set with <code>options(digits=2)</code> in the knitr preamble in your .Rnw document.  See below:</p>

<p><code>tex Typical knitr preamble
&lt;&lt;&gt;&gt;=
library(knitr)
options(digits=2)
@
</code></p>

<p>The default rounding behavior of knitr works well <em>until</em> a value contains a 0 after rounding, such as 123.10.  Running the expression <code>round(123.10,2)</code> outputs 123.1. In this case, every other value in the manuscript table would be aligned at the decimal place <em>except</em> for the unlucky value - sticking out like a sore thumb. To fix this, you <em>could</em> use <code>sprintf("%.2f", pi)</code> every time you have to call <code>\Sexpr{}</code> in the manuscript - but then what's the advantage of using <a href="http://yihui.name/knitr/">knitr</a>? This hack unnecessarily complicates the manuscript and distracts from the writing process.</p>

<h3>Modify the default inline_hook for knitr</h3>

<p>After seeing a <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">StackOverflow answer by Josh O'Brien</a>, I realized that the default inline_hook function for knitr could be easily modified to use the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a> command instead of <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html"><code>round()</code></a>.  The minute change will forcibly output all manuscript values to 2 decimal places. Below, we see the default behavior for knitr when processing inline R expressions:</p>

<p><code>r knitr's Default Hook
library(knitr)
knit_hooks$get("inline")
</code></p>

<p>```
function (x)
{</p>

<pre><code>if (is.numeric(x)) 
    x = round(x, getOption("digits"))
paste(as.character(x), collapse = ", ")
</code></pre>

<p>}
```</p>

<p><em>Note:</em> My original code for this post used the <a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/format.html"><code>format()</code></a> command.  <a href="https://github.com/wch">Winston Chang</a> pointed out that this could lead to unreliable output and tweaked the code to use <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a>. The credit for the more efficient function below goes to him.  Below, we add out improved <code>inline_hook</code> to the preamble of our knitr document:</p>

<p>```tex Improving the inline_hook
&lt;&lt;>>=
library(knitr)
inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}
knit_hooks$set(inline = inline_hook)
@
```</p>

<h3>Working Example</h3>

<p>Let's put it all together!  The following is a working example of the the suggested knitr inline_hook function, which <em>should</em> give more reliable output by rounding inline values to 2 decimal places.</p>

<p>```tex Winston's Example
\documentclass[12pt]{article}
\begin{document}</p>

<p>&lt;&lt;load, include=FALSE, echo=FALSE>>=
library(knitr)</p>

<p>inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}</p>

<p>knit_hooks$set(inline = inline_hook)
@</p>

<p>Inline code looks like \Sexpr{123}, \Sexpr{123.4}, \Sexpr{123.45}, \Sexpr{123.456}.</p>

<p>And with vectors: \Sexpr{c(123, 123.4, 123.45, 123.456)}.</p>

<p>Regular output is not affected by the inline hook:</p>

<p>&lt;&lt;>>=
123
123.4
123.45
123.456</p>

<p>c(123, 123.4, 123.45, 123.456)</p>

<p>getOption('digits')
@
\end{document}
```</p>

<p><img src="/images/example.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faster Tetrachoric Correlations]]></title>
    <link href="http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations/"/>
    <updated>2013-11-07T16:48:00-08:00</updated>
    <id>http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations</id>
    <content type="html"><![CDATA[<h2>What are tetra- and polychoric correlations?</h2>

<p>Polychoric correlations estimate the correlation between two theorized normal distributions given two ordinal variables.  In psychological research, much of our data fits this definition.  For example, many survey studies used with introductory psychology pools use Likert scale items.  The responses to these items typically range from 1 (Strongly disagree) to 6 (Strongly agree).  However, we don't <em>really</em> think that person's relationship to the item is actually polytomous.  Instead, it's an imperfect approximation.</p>

<p>Similarly, tetrachorics are special cases of polychoric crrelations when the variable of interest is dichotomous. The participant may have gotten the item either correct (i.e., 1) or incorrect (i.e., 0), but the underlying knowledge that led to the items' response is probably a continuous distribution.</p>

<p>When you have polytomous rating scales but want to disattenuate the correlations to more accurately estimate the correlation betwen the latent continuous variables, one way of doing this is to use a tetrachoric or polychoric correlation coefficient.</p>

<h2>The problem</h2>

<p>At the <a href="https://sapa-project.org">SAPA Project</a>, the majority of our data is polytomous.  We ask you the degree to which you like to go to lively parties to estimate your score on latent <em>extraversion</em>.  Presently, we use <code>mixed.cor()</code>, which calls a combination of the <code>tetrachoric()</code> and <code>polychoric()</code> functions in the <code>psych</code> package (Revelle, W., 2013).</p>

<p>However, each time we build a new dataset from the website's SQL server, it takes <em>hours</em>.  And that's <strong>if</strong> everything goes well.  If there's an error in the code or a bug in a new function, it may take hours to hit the error, wasting your day.</p>

<p>After a bit of profiling, it was revealed that much of our time building the SAPA dataset was used estimating the tetrachoric and polychoric correlation coefficients.  When you do this for 250,000+ participants for 10,000+ variables, it takes <em>a long time</em>.  So Bill and I thought about how we could speed them up and feel others may benefit from our optimization.</p>

<p>A serious speedup to tetrachoric and polychoric was initiated with the help of Bill Revelle. The increase in speed is roughly 1- (nc-1)<sup>2</sup> / nc<sup>2</sup> where nc is the number of categories. Thus, for tetrachorics where nc=2, this is a 75% reduction, whereas for polychorics of 6 item responses this is just a 30% reduction.</p>

<!-- more -->


<h3>Optimizing the existing function</h3>

<p>If we call <code>psych:::tetraBinBvn</code>, we see that most of the time spent computer the tetrachorics is using <code>pmvnorm</code> to estimate each of the 4 probabilities of the tetrachoric:</p>

<p><img src="/images/tetra.png"></p>

<p>The function says for each row <em>i</em> and each column <em>j</em>, optimize a probability based on the lower and upper bounds of the row cuts and column cuts.  <strong>But wait!</strong>  Since the sum of the probabilities in the tetrachoric 2x2 matrix add to 1, why not just use a bit of subtraction to figure it out?  That would avoid waiting on <code>pvnorm</code> to compute each cell.</p>

<p>```r Old psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
for (i in 1:2) {
    for (j in 1:2) {
        P[i, j] &lt;- pmvnorm(lower = c(row.cuts[i], col.cuts[j]), 
            upper = c(row.cuts[i + 1], col.cuts[j + 1]), 
            corr = R)
    }
}
P
</code></pre>

<p>}
```</p>

<p>```r New psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
P[1, 1] &lt;- pmvnorm(lower = c(row.cuts[1], col.cuts[1]), upper = c(row.cuts[2], 
    col.cuts[2]), corr = R)
P[1, 2] &lt;- pnorm(rc) - P[1, 1]
P[2, 1] &lt;- pnorm(cc) - P[1, 1]
P[2, 2] &lt;- 1 - pnorm(rc) - P[2, 1]
P
</code></pre>

<p>}
&lt;environment: namespace:psych>
```</p>

<p>This solution resulted in a speed up of <code>tetrachor()</code> by a factor of 3!</p>

<p>Unfortunately, our trick doesn't work for the <code>polychor()</code> function as well, since there are more degrees of freedom. However, we can eliminate calling <code>pvnorm()</code> for 11 cells in a hypothetical 6x6 matrix as we only need to find the values for 25 and can use subtraction to speed up the estimation of the remaining 11.  This results in a less impressive speed up for <code>polychor()</code> that I nonetheless hope will benefit researchers.</p>

<h3>Writing a parallelized function</h3>

<p>While we were happy with our initial speed increase, it didn't have a noticeable impact on SAPA's build time.  We then began to explore ways to parallelize our existing code.  While we attempted to use various packages (i.e., <code>foreach</code>), we settled on using <code>mcmapply</code> in the existing core parallel library to minimize our dependencies.</p>

<p>As you can see below, this resulted in a speed increase by a factor of N, where N = # of cpus you allocate.  Be warned, however, that memory consumption increases dramatically as well.</p>

<p><img src="/images/polyspeed.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyze Student Exam Items using IRT]]></title>
    <link href="http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt/"/>
    <updated>2013-10-25T14:26:00-07:00</updated>
    <id>http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt</id>
    <content type="html"><![CDATA[<p>I've cross-posted this at the <a href="http://sapa-project.org/blog/">SAPA Project's blog</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Item_response_theory">Item Response Theory</a> can be used to evaluate the effectiveness of
exams given to students.  One distinguishing feature from other paradigms is that it does not assume that every question
is equally difficult (or that the difficulty is tied to what the researcher said).  In this way, it is an empirical investigation
into the effectiveness of a given exam and can help the researcher 1) eliminate bad or problematic items and 2) judge whether the test was too difficult or the students simply didn't study.</p>

<p>In the following tutorial, we'll use <a href="http://www.r-project.org/">R</a> (R Core Team, 2013) along with the <a href="http://cran.r-project.org/web/packages/psych/index.html"><code>psych</code></a> package (Revelle, W., 2013) to look at a hypothetical exam.</p>

<!-- more -->


<p>Before we get started, remember that <code>R</code> is a programming language.  In the examples below, I perform operations on data using <a href="https://en.wikipedia.org/wiki/Function_%28computer_science%29"><em>functions</em></a> like <a href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.html"><code>cor</code></a> and <a href="http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.csv</code></a>.  We can also save the output as <a href="https://en.wikipedia.org/wiki/Object_%28computer_science%29"><em>objects</em></a> using the assignment arrow, <code>&lt;-</code>. It's a bit different from a point-and-click program like SPSS, but you <em>don't</em> need to know how to program to analyze exams and questions using IRT!</p>

<p>First, load the the <code>psych</code> package.  Next, load the students' grades into R using <code>read.csv()</code> from the <code>psych</code> package.</p>

<p>```r</p>

<h1>Load the psych package</h1>

<p>library(psych)</p>

<h1>Read the csv file and save it as grades</h1>

<p>grades &lt;- read.csv(file = "~/Downloads/Grades.csv")
```</p>

<p>Notice that we are using item-level grades, where each row is a given student and each cell is the number of points received on that question.  In my example, V1, V2, etc. correspond to exam questions.  Your matrix or data frame should look like this:</p>

<p><code>r
head(grades)
</code></p>

<p>```</p>

<h2>V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20</h2>

<h2>1  2  2  2  2  4  2  3  2  4   2   2   3   5   3   6   2   4   4   4   3</h2>

<h2>2  2  0  2  2  0  0  3  0  4   2   0   1   3   0   0   2   2   1   0   0</h2>

<h2>3  2  2  2  2  0  2  3  2  4   0   0   3   5   0   5   2   4   2   4   2</h2>

<h2>4  2  1  2  2  3  0  0  2  4   2   2   3   1   1   2   0   4   1   4   3</h2>

<h2>5  2  2  2  2  4  2  3  2  4   2   2   3   5   3   4   2   4   4   4   3</h2>

<h2>6  1  0  2  2  0  2  0  2  0   2   1   2   5   0   3   2   4   2   4   3</h2>

<h2>V21 V22 V23 V24 V25 V26 V27 Total</h2>

<h2>1   2   2   4   4   6   6   6    91</h2>

<h2>2   0   0   0   3   3   6   6    42</h2>

<h2>3   2   1   2   4   6   5   6    72</h2>

<h2>4   0   2   4   4   0   0   0    49</h2>

<h2>5   2   2   4   4   5   6   6    88</h2>

<h2>6   0   0   4   4   6   4   3    58</h2>

<p>```</p>

<p>Next, compute the <a href="https://en.wikipedia.org/wiki/Polychoric_correlation">polychoric correlations</a> on the raw grades (not including the Total column).  By using polychoric correlations, we estimate the normal distribution of latent content knowledge, which can be underestimated if Pearson correlations are instead used on polytomous items (see  Lee, Poon &amp; Bentler, 1995).</p>

<p>```r</p>

<h1>Perform a polychoric correlation on grades and save it as grades.poly</h1>

<p>grades.poly &lt;- polychoric(x = grades[, 1:27], polycor = TRUE)
```</p>

<p>Now that we have the polychoric correlations, we can run <code>irt.fa()</code> on the dataset to see the item difficulties and information.</p>

<p>```r</p>

<h1>Using grades.poly, perform an irt and save it as grades.irt</h1>

<p>grades.irt &lt;- irt.fa(x = grades.poly, plot = TRUE)</p>

<h1>Plot the output from grades.irt</h1>

<p>plot(grades.irt)
```</p>

<p><img src="/images/grades.png"></p>

<p>Thus, we have some great items that have a lot of information about students of average and low content knowledge (e.g., V24, V17, V18), but not enough to distinguish the high-knowledge students.  In redesigning an exam for next semester or year, we might save the best performing questions while trying to rewrite the existing questions or trying new questions.</p>

<p>Next, let's see what how well the test did <em>overall</em> at distinguishing students:</p>

<p><code>r
plot.irt(type = "test", grades.irt)
</code></p>

<p><img src="/images/test.png"></p>

<p>The second plot shows the <em>test performance</em>.  We have great reliability for distinguishing who didn't study (lowers end of our latent trait), but overall the test may have been too easy (opposite of my prediction).  It's important that the test is not too difficult to discourage students, but the graph above suggests that we had very low information at how students that studied were different from eachother.  This is again reflected by the histogram plotted in the next section, where high scoring students seem to cluster together.</p>

<h2>Rescaling the test</h2>

<p>While many students in our hypothetical dataset did very well on the exam, instructors may
need to rescale their exam so that the mean grade is an 85% or 87.5%.  Using the <code>scale()</code> function (see also <a href="http://personality-project.org/r/psych/help/rescale.html"><code>rescale()</code></a> in <code>psych</code> package), we
can ensure that the <a href="https://en.wikipedia.org/wiki/Ranking#Ranking_in_statistics?s">rank-order</a> distribution of the students is preserved (allowing us to distinguish
those who studied well from those who didn't), while scaling the sample distribution to fit in with other classes in your department.</p>

<p>Currently, our scores are in cumulative raw points.  Notice that we divide the Total points column by 91 to convert the histogram into grade percentages.  Let's plot a histogram to see the distribution of scores.</p>

<p>```r</p>

<h1>Load the ggplot2 library</h1>

<p>library(ggplot2)</p>

<h1>Plot a histogram</h1>

<p>qplot((Total/91)*100, data=grades, geom="histogram",xlab='Raw Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/raw.png"></p>

<p>The distribution has a mean 71.68 percent and a standard deviation of 15.54.  Given grade inflation,
it may look like your students are doing poorly when in fact the distribution is similiar to other courses
being taught.  Next, we can rescale the grades, creating a mean of 87.5 and a standard deviation of 7.5.  These numbers are arbitrary so use your best judgement.</p>

<p>```r
scaled.grades &lt;- scale(grades$Total) * 7.5 + 87.5
qplot(scaled.grades, xlab='Scaled Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/scaled.png"></p>

<p>The second distribution may be preferred, depending on your needs.  With the raw distribution, we would have had 45% of the students receiving grades below a C-, assuming a normal distribution (for the curious, R can calculate these probabilites using the <code>pnorm</code> function: <code>pnorm(q=70,mean=71.68,sd=15.54)</code>).  Now, 0.9% of students would fall below the 70% cutoff.  Again, my mean and standard deviation chosen in the above example are arbitrary.</p>

<h2>References</h2>

<ul>
<li><p>Lee, S. Y., Poon, W. Y., &amp; Bentler, P. M. (1995). <em>A two‐stage estimation of structural equation models with continuous and polytomous variables</em>. British Journal of Mathematical and Statistical Psychology, 48(2), 339-358.</p></li>
<li><p>R Core Team (2013). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing, Vienna, Austria. <a href="http://www.R-project.org/">http://www.R-project.org/</a>.</p></li>
<li><p>Revelle, W. (2013). <em>psych: Procedures for Personality and Psychological Research</em>. Northwestern University, Evanston, Illinois, USA. <a href="http://CRAN.R-project.org/package=psych">http://CRAN.R-project.org/package=psych</a>. Version = 1.3.10.</p></li>
</ul>

]]></content>
  </entry>
  
</feed>
