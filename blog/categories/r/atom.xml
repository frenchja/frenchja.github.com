<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: R | Jason A. French]]></title>
  <link href="http://frenchja.github.com/blog/categories/r/atom.xml" rel="self"/>
  <link href="http://frenchja.github.com/"/>
  <updated>2014-05-28T16:19:03-05:00</updated>
  <id>http://frenchja.github.com/</id>
  <author>
    <name><![CDATA[Jason A. French]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Fixing knitr: Formatting statistical output to 2 digits in R]]></title>
    <link href="http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits/"/>
    <updated>2014-04-25T16:53:00-05:00</updated>
    <id>http://frenchja.github.com/blog/2014/04/25/formatting-sweave-and-knitr-output-for-2-digits</id>
    <content type="html"><![CDATA[<h3>Overview of reproducible research</h3>

<p>Reproducible research is a phrase that describes an academic paper or manuscript that contains the code and data in addition to what is usually published - the researcher's interpretation.  In doing so, the experimental design and method of analysis is easily replicated by unaffiliated labs and critiqued by reviewers as the full analysis used to produce the results is submitted along with the final paper.  One way of producing reproducible research is to use <a href="http://r-project.org">R code</a> directly inside your LaTeX document. In order to faciliate the combination of statistical code and manuscript writing, two R packages in particular have arisen:  <a href="">Sweave</a> and <a href="">knitr</a>. knitr is an R package designed as a replacement for Sweave, but both packages combine your R analysis with your <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a> manuscript (i.e., knitr = R + LaTeX).</p>

<p>One advantage of knitr is that the researcher can easily create ANOVA and demographic tables directly from the data without messing around in Excel.  However, as we'll see, both knitr and Sweave can run into problems when formatting your table values to 2 decimal points.  In this post, I'll detail my proposed method of fixing that which can be applied to your entire mansucript by editing the beginning of your knitr preamble.</p>

<!-- more -->


<p>The basic example below contains the beginning of a hypothetical Methods section of a manuscript. We want to take the values from an R table, which has the breakdown of participants by gender and ethnicity, and display them as numbers in our manuscript.</p>

<p>```tex Basic knitr.Rnw Example
\documentclass[12pt]{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}</p>

<p>&lt;&lt;setup, include=FALSE, cache=FALSE>>=
library(knitr)</p>

<h1>set global chunk options</h1>

<p>opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=90)
@</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>&lt;&lt;random-ethnicity, include=FALSE>>=</p>

<h1>Create data.frame of random ethnicities</h1>

<p>x &lt;- data.frame(Ethnicity = sample(as.factor(
  rep(x = c('White','African American','Asian American', 'Latino', 'Pacific Islander')</p>

<pre><code>)
</code></pre>

<p>  ),size = 100, replace = TRUE),
  Gender=rep(c('Male', 'Female'),100))
x.table &lt;- table(x)
@</p>

<p>\section{Methods}
We recruited \Sexpr{sum(x.table)} university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>&lt;&lt;ethnicity-table, echo = FALSE, results = 'asis'>>=
library(xtable)
xtable(x.table, caption = 'Participant Ethnicities', label='tab:ethnicity')
@</p>

<p>\end{document}
```</p>

<p>As we see below, running the <code>knit()</code> command on our knitr manuscript inside R produces a regular LaTeX file that can be compiled with to a PDF using pdflatex or <a href="http://pages.uoregon.edu/koch/texshop/">TeX Shop</a>. <em>Notice that the R table objects have been replaced with LaTeX tables.</em></p>

<p><code>r Running knit() knitr.Rnw inside R
library(knitr)
knit(input = 'knitr.Rnw', output = 'knitr.tex')
</code></p>

<p>```tex Resulting knitr.tex LaTeX document
\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
\makeatletter
\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}</p>

<p>\usepackage{framed}</p>

<p>\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX</p>

<p>\usepackage{alltt}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}</p>

<p>\title{A Minimal Demo of knitr}
\author{Jason A. French}
\maketitle</p>

<p>\section{Methods}
We recruited 200 university undergraduates from an introductory psychology class. Participants were drawn from various genders and ethnic groups across the Chicago area (see Table~\ref{tab:ethnicity})...</p>

<p>\begin{table}[ht]
\centering
\begin{tabular}{rrr}
\hline
 &amp; Female &amp; Male \
\hline
African American &amp;  22 &amp;  16 \
Asian American &amp;  26 &amp;  30 \
Latino &amp;  14 &amp;  30 \
Pacific Islander &amp;  18 &amp;  14 \
White &amp;  20 &amp;  10 \
\hline
\end{tabular}
\caption{Participant Ethnicities}
\label{tab:ethnicity}
\end{table}
\end{document}
```</p>

<p>Last, after compiling our LaTeX file using TeX Shop, we're greeted with the final product below:</p>

<p><img src="/images/knitr-example.png"></p>

<h3>Summary thus far</h3>

<p>The example above used data from R directly in a sentence in the Methods section (i.e., "We recruited 200 university undergraduates from an introductory psychology class.") and did so using the <code>\Sexpr{}</code> command in the <a href="https://en.wikipedia.org/wiki/LaTeX">knitr</a> manuscript (i.e., knitr.Rnw).  The <code>\Sexpr{}</code> command contained an <code>R</code> expression to calculate the total number participants.  This expression was evaluated and converted to LaTeX code when we ran the <code>knit()</code> function on the .Rnw file, which produces a .tex document. The .tex document contained no R code and was therefore ready to be compiled to a PDF using TeX Shop or pdf2latex in Terminal.app.</p>

<h2>Forcing knitr to round to 2 decimal places</h2>

<p>The default behavior of knitr works well <em>most</em> of the time. However, what if we didn't have whole numbers in our data table?  What if we had percentages that we wanted to round down to 2 digits, as required by many journals?  For example, the value <code>\Sexpr{pi}</code> would be evaluated and replaced with 3.141593 in the LaTeX file.  One common problem, and part of <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">Yihui's motivation</a> for replacing Sweave with <a href="http://yihui.name/knitr/"><code>knitr</code></a>, is that <code>\Sexpr{}</code> doesn't automatically round digits.</p>

<p>In Sweave (i.e., knitr's predecessor), <em>each</em> value of pi would have to be encased in <code>round(pi,2)</code>.  Thus, we end up with <code>\Sexpr{round(pi,2)}</code>.  Yihui fixed this problem by automatically rounding digits, the length of which is set with <code>options(digits=2)</code> in the knitr preamble in your .Rnw document.  See below:</p>

<p><code>tex Typical knitr preamble
&lt;&lt;&gt;&gt;=
library(knitr)
options(digits=2)
@
</code></p>

<p>The default rounding behavior of knitr works well <em>until</em> a value contains a 0 after rounding, such as 123.10.  Running the expression <code>round(123.10,2)</code> outputs 123.1. In this case, every other value in the manuscript table would be aligned at the decimal place <em>except</em> for the unlucky value - sticking out like a sore thumb. To fix this, you <em>could</em> use <code>sprintf("%.2f", pi)</code> every time you have to call <code>\Sexpr{}</code> in the manuscript - but then what's the advantage of using <a href="http://yihui.name/knitr/">knitr</a>? This hack unnecessarily complicates the manuscript and distracts from the writing process.</p>

<h3>Modify the default inline_hook for knitr</h3>

<p>After seeing a <a href="https://stackoverflow.com/questions/11062497/how-to-avoid-using-round-in-every-sexpr">StackOverflow answer by Josh O'Brien</a>, I realized that the default inline_hook function for knitr could be easily modified to use the <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a> command instead of <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/Round.html"><code>round()</code></a>.  The minute change will forcibly output all manuscript values to 2 decimal places. Below, we see the default behavior for knitr when processing inline R expressions:</p>

<p><code>r knitr's Default Hook
library(knitr)
knit_hooks$get("inline")
</code></p>

<p>```
function (x)
{</p>

<pre><code>if (is.numeric(x)) 
    x = round(x, getOption("digits"))
paste(as.character(x), collapse = ", ")
</code></pre>

<p>}
```</p>

<p><em>Note:</em> My original code for this post used the <a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/format.html"><code>format()</code></a> command.  <a href="https://github.com/wch">Winston Chang</a> pointed out that this could lead to unreliable output and tweaked the code to use <a href="http://stat.ethz.ch/R-manual/R-devel/library/base/html/sprintf.html"><code>sprintf()</code></a>. The credit for the more efficient function below goes to him.  Below, we add out improved <code>inline_hook</code> to the preamble of our knitr document:</p>

<p>```tex Improving the inline_hook
&lt;&lt;>>=
library(knitr)
inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}
knit_hooks$set(inline = inline_hook)
@
```</p>

<h3>Working Example</h3>

<p>Let's put it all together!  The following is a working example of the the suggested knitr inline_hook function, which <em>should</em> give more reliable output by rounding inline values to 2 decimal places.</p>

<p>```tex Winston's Example
\documentclass[12pt]{article}
\begin{document}</p>

<p>&lt;&lt;load, include=FALSE, echo=FALSE>>=
library(knitr)</p>

<p>inline_hook &lt;- function (x) {
  if (is.numeric(x)) {</p>

<pre><code># ifelse does a vectorized comparison
# If integer, print without decimal; otherwise print two places
res &lt;- ifelse(x == round(x),
  sprintf("%d", x),
  sprintf("%.2f", x)
)
paste(res, collapse = ", ")
</code></pre>

<p>  }
}</p>

<p>knit_hooks$set(inline = inline_hook)
@</p>

<p>Inline code looks like \Sexpr{123}, \Sexpr{123.4}, \Sexpr{123.45}, \Sexpr{123.456}.</p>

<p>And with vectors: \Sexpr{c(123, 123.4, 123.45, 123.456)}.</p>

<p>Regular output is not affected by the inline hook:</p>

<p>&lt;&lt;>>=
123
123.4
123.45
123.456</p>

<p>c(123, 123.4, 123.45, 123.456)</p>

<p>getOption('digits')
@
\end{document}
```</p>

<p><img src="/images/example.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Faster Tetrachoric Correlations]]></title>
    <link href="http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations/"/>
    <updated>2013-11-07T16:48:00-06:00</updated>
    <id>http://frenchja.github.com/blog/2013/11/07/faster-tetrachoric-correlations</id>
    <content type="html"><![CDATA[<h2>What are tetra- and polychoric correlations?</h2>

<p>Polychoric correlations estimate the correlation between two theorized normal distributions given two ordinal variables.  In psychological research, much of our data fits this definition.  For example, many survey studies used with introductory psychology pools use Likert scale items.  The responses to these items typically range from 1 (Strongly disagree) to 6 (Strongly agree).  However, we don't <em>really</em> think that person's relationship to the item is actually polytomous.  Instead, it's an imperfect approximation.</p>

<p>Similarly, tetrachorics are special cases of polychoric crrelations when the variable of interest is dichotomous. The participant may have gotten the item either correct (i.e., 1) or incorrect (i.e., 0), but the underlying knowledge that led to the items' response is probably a continuous distribution.</p>

<p>When you have polytomous rating scales but want to disattenuate the correlations to more accurately estimate the correlation betwen the latent continuous variables, one way of doing this is to use a tetrachoric or polychoric correlation coefficient.</p>

<h2>The problem</h2>

<p>At the <a href="https://sapa-project.org">SAPA Project</a>, the majority of our data is polytomous.  We ask you the degree to which you like to go to lively parties to estimate your score on latent <em>extraversion</em>.  Presently, we use <code>mixed.cor()</code>, which calls a combination of the <code>tetrachoric()</code> and <code>polychoric()</code> functions in the <code>psych</code> package (Revelle, W., 2013).</p>

<p>However, each time we build a new dataset from the website's SQL server, it takes <em>hours</em>.  And that's <strong>if</strong> everything goes well.  If there's an error in the code or a bug in a new function, it may take hours to hit the error, wasting your day.</p>

<p>After a bit of profiling, it was revealed that much of our time building the SAPA dataset was used estimating the tetrachoric and polychoric correlation coefficients.  When you do this for 250,000+ participants for 10,000+ variables, it takes <em>a long time</em>.  So Bill and I thought about how we could speed them up and feel others may benefit from our optimization.</p>

<p>A serious speedup to tetrachoric and polychoric was initiated with the help of Bill Revelle. The increase in speed is roughly 1- (nc-1)<sup>2</sup> / nc<sup>2</sup> where nc is the number of categories. Thus, for tetrachorics where nc=2, this is a 75% reduction, whereas for polychorics of 6 item responses this is just a 30% reduction.</p>

<!-- more -->


<h3>Optimizing the existing function</h3>

<p>If we call <code>psych:::tetraBinBvn</code>, we see that most of the time spent computer the tetrachorics is using <code>pmvnorm</code> to estimate each of the 4 probabilities of the tetrachoric:</p>

<p><img src="/images/tetra.png"></p>

<p>The function says for each row <em>i</em> and each column <em>j</em>, optimize a probability based on the lower and upper bounds of the row cuts and column cuts.  <strong>But wait!</strong>  Since the sum of the probabilities in the tetrachoric 2x2 matrix add to 1, why not just use a bit of subtraction to figure it out?  That would avoid waiting on <code>pvnorm</code> to compute each cell.</p>

<p>```r Old psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
for (i in 1:2) {
    for (j in 1:2) {
        P[i, j] &lt;- pmvnorm(lower = c(row.cuts[i], col.cuts[j]), 
            upper = c(row.cuts[i + 1], col.cuts[j + 1]), 
            corr = R)
    }
}
P
</code></pre>

<p>}
```</p>

<p>```r New psych:::tetraBinBvn
function (rho, rc, cc)
{</p>

<pre><code>row.cuts &lt;- c(-Inf, rc, Inf)
col.cuts &lt;- c(-Inf, cc, Inf)
P &lt;- matrix(0, 2, 2)
R &lt;- matrix(c(1, rho, rho, 1), 2, 2)
P[1, 1] &lt;- pmvnorm(lower = c(row.cuts[1], col.cuts[1]), upper = c(row.cuts[2], 
    col.cuts[2]), corr = R)
P[1, 2] &lt;- pnorm(rc) - P[1, 1]
P[2, 1] &lt;- pnorm(cc) - P[1, 1]
P[2, 2] &lt;- 1 - pnorm(rc) - P[2, 1]
P
</code></pre>

<p>}
&lt;environment: namespace:psych>
```</p>

<p>This solution resulted in a speed up of <code>tetrachor()</code> by a factor of 3!</p>

<p>Unfortunately, our trick doesn't work for the <code>polychor()</code> function as well, since there are more degrees of freedom. However, we can eliminate calling <code>pvnorm()</code> for 11 cells in a hypothetical 6x6 matrix as we only need to find the values for 25 and can use subtraction to speed up the estimation of the remaining 11.  This results in a less impressive speed up for <code>polychor()</code> that I nonetheless hope will benefit researchers.</p>

<h3>Writing a parallelized function</h3>

<p>While we were happy with our initial speed increase, it didn't have a noticeable impact on SAPA's build time.  We then began to explore ways to parallelize our existing code.  While we attempted to use various packages (i.e., <code>foreach</code>), we settled on using <code>mcmapply</code> in the existing core parallel library to minimize our dependencies.</p>

<p>As you can see below, this resulted in a speed increase by a factor of N, where N = # of cpus you allocate.  Be warned, however, that memory consumption increases dramatically as well.</p>

<p><img src="/images/polyspeed.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyze Student Exam Items using IRT]]></title>
    <link href="http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt/"/>
    <updated>2013-10-25T14:26:00-05:00</updated>
    <id>http://frenchja.github.com/blog/2013/10/25/analyzing-student-exams-using-irt</id>
    <content type="html"><![CDATA[<p>I've cross-posted this at the <a href="http://sapa-project.org/blog/">SAPA Project's blog</a>.</p>

<p><a href="https://en.wikipedia.org/wiki/Item_response_theory">Item Response Theory</a> can be used to evaluate the effectiveness of
exams given to students.  One distinguishing feature from other paradigms is that it does not assume that every question
is equally difficult (or that the difficulty is tied to what the researcher said).  In this way, it is an empirical investigation
into the effectiveness of a given exam and can help the researcher 1) eliminate bad or problematic items and 2) judge whether the test was too difficult or the students simply didn't study.</p>

<p>In the following tutorial, we'll use <a href="http://www.r-project.org/">R</a> (R Core Team, 2013) along with the <a href="http://cran.r-project.org/web/packages/psych/index.html"><code>psych</code></a> package (Revelle, W., 2013) to look at a hypothetical exam.</p>

<!-- more -->


<p>Before we get started, remember that <code>R</code> is a programming language.  In the examples below, I perform operations on data using <a href="https://en.wikipedia.org/wiki/Function_%28computer_science%29"><em>functions</em></a> like <a href="http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.html"><code>cor</code></a> and <a href="http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html"><code>read.csv</code></a>.  We can also save the output as <a href="https://en.wikipedia.org/wiki/Object_%28computer_science%29"><em>objects</em></a> using the assignment arrow, <code>&lt;-</code>. It's a bit different from a point-and-click program like SPSS, but you <em>don't</em> need to know how to program to analyze exams and questions using IRT!</p>

<p>First, load the the <code>psych</code> package.  Next, load the students' grades into R using <code>read.csv()</code> from the <code>psych</code> package.</p>

<p>```r</p>

<h1>Load the psych package</h1>

<p>library(psych)</p>

<h1>Read the csv file and save it as grades</h1>

<p>grades &lt;- read.csv(file = "~/Downloads/Grades.csv")
```</p>

<p>Notice that we are using item-level grades, where each row is a given student and each cell is the number of points received on that question.  In my example, V1, V2, etc. correspond to exam questions.  Your matrix or data frame should look like this:</p>

<p><code>r
head(grades)
</code></p>

<p>```</p>

<h2>V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20</h2>

<h2>1  2  2  2  2  4  2  3  2  4   2   2   3   5   3   6   2   4   4   4   3</h2>

<h2>2  2  0  2  2  0  0  3  0  4   2   0   1   3   0   0   2   2   1   0   0</h2>

<h2>3  2  2  2  2  0  2  3  2  4   0   0   3   5   0   5   2   4   2   4   2</h2>

<h2>4  2  1  2  2  3  0  0  2  4   2   2   3   1   1   2   0   4   1   4   3</h2>

<h2>5  2  2  2  2  4  2  3  2  4   2   2   3   5   3   4   2   4   4   4   3</h2>

<h2>6  1  0  2  2  0  2  0  2  0   2   1   2   5   0   3   2   4   2   4   3</h2>

<h2>V21 V22 V23 V24 V25 V26 V27 Total</h2>

<h2>1   2   2   4   4   6   6   6    91</h2>

<h2>2   0   0   0   3   3   6   6    42</h2>

<h2>3   2   1   2   4   6   5   6    72</h2>

<h2>4   0   2   4   4   0   0   0    49</h2>

<h2>5   2   2   4   4   5   6   6    88</h2>

<h2>6   0   0   4   4   6   4   3    58</h2>

<p>```</p>

<p>Next, compute the <a href="https://en.wikipedia.org/wiki/Polychoric_correlation">polychoric correlations</a> on the raw grades (not including the Total column).  By using polychoric correlations, we estimate the normal distribution of latent content knowledge, which can be underestimated if Pearson correlations are instead used on polytomous items (see  Lee, Poon &amp; Bentler, 1995).</p>

<p>```r</p>

<h1>Perform a polychoric correlation on grades and save it as grades.poly</h1>

<p>grades.poly &lt;- polychoric(x = grades[, 1:27], polycor = TRUE)
```</p>

<p>Now that we have the polychoric correlations, we can run <code>irt.fa()</code> on the dataset to see the item difficulties and information.</p>

<p>```r</p>

<h1>Using grades.poly, perform an irt and save it as grades.irt</h1>

<p>grades.irt &lt;- irt.fa(x = grades.poly, plot = TRUE)</p>

<h1>Plot the output from grades.irt</h1>

<p>plot(grades.irt)
```</p>

<p><img src="/images/grades.png"></p>

<p>Thus, we have some great items that have a lot of information about students of average and low content knowledge (e.g., V24, V17, V18), but not enough to distinguish the high-knowledge students.  In redesigning an exam for next semester or year, we might save the best performing questions while trying to rewrite the existing questions or trying new questions.</p>

<p>Next, let's see what how well the test did <em>overall</em> at distinguishing students:</p>

<p><code>r
plot.irt(type = "test", grades.irt)
</code></p>

<p><img src="/images/test.png"></p>

<p>The second plot shows the <em>test performance</em>.  We have great reliability for distinguishing who didn't study (lowers end of our latent trait), but overall the test may have been too easy (opposite of my prediction).  It's important that the test is not too difficult to discourage students, but the graph above suggests that we had very low information at how students that studied were different from eachother.  This is again reflected by the histogram plotted in the next section, where high scoring students seem to cluster together.</p>

<h2>Rescaling the test</h2>

<p>While many students in our hypothetical dataset did very well on the exam, instructors may
need to rescale their exam so that the mean grade is an 85% or 87.5%.  Using the <code>scale()</code> function (see also <a href="http://personality-project.org/r/psych/help/rescale.html"><code>rescale()</code></a> in <code>psych</code> package), we
can ensure that the <a href="https://en.wikipedia.org/wiki/Ranking#Ranking_in_statistics?s">rank-order</a> distribution of the students is preserved (allowing us to distinguish
those who studied well from those who didn't), while scaling the sample distribution to fit in with other classes in your department.</p>

<p>Currently, our scores are in cumulative raw points.  Notice that we divide the Total points column by 91 to convert the histogram into grade percentages.  Let's plot a histogram to see the distribution of scores.</p>

<p>```r</p>

<h1>Load the ggplot2 library</h1>

<p>library(ggplot2)</p>

<h1>Plot a histogram</h1>

<p>qplot((Total/91)*100, data=grades, geom="histogram",xlab='Raw Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/raw.png"></p>

<p>The distribution has a mean 71.68 percent and a standard deviation of 15.54.  Given grade inflation,
it may look like your students are doing poorly when in fact the distribution is similiar to other courses
being taught.  Next, we can rescale the grades, creating a mean of 87.5 and a standard deviation of 7.5.  These numbers are arbitrary so use your best judgement.</p>

<p>```r
scaled.grades &lt;- scale(grades$Total) * 7.5 + 87.5
qplot(scaled.grades, xlab='Scaled Grades',</p>

<pre><code>ylab='# of Students',main='Distribution of student grades')
</code></pre>

<p>```</p>

<p><img src="/images/scaled.png"></p>

<p>The second distribution may be preferred, depending on your needs.  With the raw distribution, we would have had 45% of the students receiving grades below a C-, assuming a normal distribution (for the curious, R can calculate these probabilites using the <code>pnorm</code> function: <code>pnorm(q=70,mean=71.68,sd=15.54)</code>).  Now, 0.9% of students would fall below the 70% cutoff.  Again, my mean and standard deviation chosen in the above example are arbitrary.</p>

<h2>References</h2>

<ul>
<li><p>Lee, S. Y., Poon, W. Y., &amp; Bentler, P. M. (1995). <em>A two‚Äêstage estimation of structural equation models with continuous and polytomous variables</em>. British Journal of Mathematical and Statistical Psychology, 48(2), 339-358.</p></li>
<li><p>R Core Team (2013). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing, Vienna, Austria. <a href="http://www.R-project.org/">http://www.R-project.org/</a>.</p></li>
<li><p>Revelle, W. (2013). <em>psych: Procedures for Personality and Psychological Research</em>. Northwestern University, Evanston, Illinois, USA. <a href="http://CRAN.R-project.org/package=psych">http://CRAN.R-project.org/package=psych</a>. Version = 1.3.10.</p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Easy Sweave for LaTeX and R]]></title>
    <link href="http://frenchja.github.com/blog/2013/08/16/easy-sweaving-for-latex-and-r/"/>
    <updated>2013-08-16T12:41:00-05:00</updated>
    <id>http://frenchja.github.com/blog/2013/08/16/easy-sweaving-for-latex-and-r</id>
    <content type="html"><![CDATA[<p>When you're writing up reports using statistics from R, it can be tiresome
to constantly copy and paste results from the R Console.  To get around this, many of us use Sweave, which allows us to <em>embed</em> R code in LaTeX files.
<a href="https://en.wikipedia.org/wiki/Sweave">Sweave</a> is an R function that converts R code to LaTeX, a document typesetting language.  This enables accurate, shareable analyses as well as high-resolution graphs that are publication quality.</p>

<p>Needless to say, the marriage of statistics with documents makes writing up APA-style reports a bit easier, especially with Brian Beitzel's amazing <a href="http://www.ctan.org/pkg/apa6"><code>apa6</code> class for LaTeX</a>.</p>

<!-- more -->


<h2>Making Sweave Available Systemwide</h2>

<p>However, Sweave doesn't always work correctly.  One common complaint that you'll get after Sweaving a file is <code>Sweave.sty not found!</code>. While Sweave.sty is a LaTeX package, it doesn't <em>live</em> with the rest of the LaTeX packages because it's installed using R.  Many people try to solve this by copying and pasting Sweave.sty into every document directory, but I'm sharing a better way below.</p>

<p>Using <a href="https://en.wikipedia.org/wiki/Terminal_%28OS_X%29">Terminal.app</a>:
<code>bash Go to the MacTeX or TeX Live local directory.
cd /usr/local/texlive/texmf-local/tex/latex
</code></p>

<p><code>bash Form a link between the R Sweave.sty files and MacTeX
sudo ln -s /Library/Frameworks/R.framework/Resources/share/texmf/ Sweave
</code></p>

<p><code>bash Tell MacTeX/TeX Live to recognize the file and rebuild the database
sudo mktexlsr
</code></p>

<p>If using <code>mktexlsr</code> results in a <code>command not found</code> error, the TeX Live distribution probably isn't in your <a href="https://en.wikipedia.org/wiki/PATH_%28variable%29">$PATH</a>, but you can hunt for the program anyway.  For example, if you're using MacTeX 2013, the program will be found in a directory similar to this:</p>

<p><code>bash
sudo /usr/local/texlive/2013/bin/x86_64-darwin/mktexlsr
</code></p>

<h2>Using Sweave in TeXShop</h2>

<p>When you're getting started with LaTeX, many Mac users prefer the bundled
editor, TeXShop.  <a href="http://cameron.bracken.bz/sweave-for-texshop">Cameron Bracken</a> gives us a helpful piece of code that allows easy Sweaving straight from TeXShop.  TeXShop uses various <em>engines</em> that allow it to render LaTeX.  Using a bit of <a href="https://en.wikipedia.org/wiki/Bash_%28Unix_shell%29"><code>BASH</code></a> scripting, we can write our own Sweave engine and make it available right within TeXShop.  I have adapted Cameron's original engine to accomodate Bibtex citations (<a href="https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management#Why_won.27t_LaTeX_generate_any_output.3F">see here</a>).</p>

<p>Using a text editor, paste in the following syntax and save the file as Sweave.engine:</p>

<p>```bash Sweave.engine</p>

<h1>!/bin/env bash</h1>

<p>export PATH=$PATH:/usr/texbin:/usr/local/bin
R CMD Sweave "$1"
pdflatex "${1%.<em>}"
bibtex "${1%.</em>}.aux"
pdflatex "${1%.*}"</p>

<h1>If you run pdflatex again you get citations</h1>

<p>pdflatex "${1%.*}"
```</p>

<p>Next, in Terminal.app, move the file to the TeXShop engines folder:</p>

<p><code>bash Enable the Sweave.engine
mv Sweave.engine ~/Library/TeXShop/Engines/
chmod +x ~/Library/TeXShop/Engines/Sweave.engine
</code></p>

<p><img src="/images/sweaveengine.png"></p>

<p>Now restart TeXShop if it's running and you should see Sweave as an available option!</p>

<p><img src="/images/sweave.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing R in Linux]]></title>
    <link href="http://frenchja.github.com/blog/2013/03/11/installing-r-in-linux/"/>
    <updated>2013-03-11T17:18:00-05:00</updated>
    <id>http://frenchja.github.com/blog/2013/03/11/installing-r-in-linux</id>
    <content type="html"><![CDATA[<p>This guide is intended to faciliate the installation of up-to-date R packages
for users new to either R or Linux.  Unlike Windows binaries or Mac packages,
Linux software is often distributed as source-code and then compiled by package
maintainers.  The use of package managers has many advantages that I won't
discuss here (see <a href="https://en.wikipedia.org/wiki/Package_management_system">Wikipedia</a>).<br/>
More importantly, the difference can be initially intimidating.<br/>
However, once the user gets used to using package managers such as
<a href="https://en.wikipedia.org/wiki/Advanced_Packaging_Tool">apt</a> or
<a href="https://en.wikipedia.org/wiki/Yellowdog_Updater,_Modified">yum</a> to install software,
I'm confident they'll appreciate their ease of use.</p>

<p>These instructions are organized by system type.</p>

<h2>Debian-based Distributions</h2>

<h3>Ubuntu</h3>

<p>Full installation instructions for Ubuntu can be found
<a href="http://cran.rstudio.com/bin/linux/ubuntu/">here</a>.  Luckily, CRAN mirrors have
compiled binaries of R which can be installed using the apt-get package manager.
To accomplish this, we'll first add the <a href="http://cran.us.r-project.org/bin/linux/ubuntu/">CRAN
repo</a> for Ubuntu packages to
<code>/etc/apt/sources.list</code>.  If you prefer to manually edit the sources.list file,
you can do so by issuing the following in the terminal:</p>

<p><code>bash Inspecting sources.list
sudo nano /etc/apt/sources.list
</code></p>

<p>``` bash Installing R in Ubuntu</p>

<h1>Grabs your version of Ubuntu as a BASH variable</h1>

<p>CODENAME=<code>grep CODENAME /etc/lsb-release | cut -c 18-</code></p>

<h1>Appends the CRAN repository to your sources.list file</h1>

<p>sudo sh -c 'echo "deb http://cran.rstudio.com/bin/linux/ubuntu $CODENAME" >> /etc/apt/sources.list'</p>

<h1>Adds the CRAN GPG key, which is used to sign the R packages for security.</h1>

<p>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9</p>

<p>sudo apt-get update
sudo apt-get install r-base r-dev
```</p>

<!-- more -->


<h3>Debian</h3>

<p>The instructions for installing R in Debian are similar to Ubuntu.  Regarding 'stable'
versions of Debian, the CRAN
<a href="http://cran.rstudio.com/bin/linux/debian/README.html">README file</a> for Debian
points out:</p>

<p><blockquote><p>After a release of Debian "stable", no new packages get added by Debian to keep the release as 'stable' as possible. This implies that the R release contained in the official Debian release will become outdated as time passes.</p><footer><strong>CRAN</strong> <cite><a href='http://cran.us.r-project.org/bin/linux/debian/README.html'>README</a></cite></footer></blockquote></p>

<p>Thus, we'll append the CRAN repository to the Debian list to update the available R version, just like
we did for Ubuntu:</p>

<p>``` bash Installing R in Debian Stable</p>

<h1>Appends the CRAN repository to your sources.list file</h1>

<p>sudo sh -c 'echo "deb http://cran.rstudio.com/bin/linux/debian lenny-cran/" >> /etc/apt/sources.list'</p>

<h1>Adds the CRAN GPG key, which is used to sign the R packages for security.</h1>

<p>sudo apt-key adv --keyserver subkeys.pgp.net --recv-key 381BA480
sudo apt-get update
sudo apt-get install r-base r-base-dev
```</p>

<p>Finally, you can search for additional R packages in terminal using <code>apt-cache</code>:
<code>bash Searching for R packages using apt-get
apt-cache search ^r-.*
</code></p>

<h3>Installing RStudio</h3>

<p>Rstudio is a cross-platform user interface for R. The
RStudio package is compiled for both Debian and Ubuntu distributions.
Therefore, the installation instructions are the same.  Copy the link for the
latest RStudio package from <a href="http:/%0A/www.rstudio.com/ide/download/desktop">http://www.rstudio.com/ide/download/desktop</a> (e.g., <a href="http://download1.rstudio.org/rstudio-0.97.320-amd64.deb">http://download1.rstudio.org/rstudio-0.97.320-amd64.deb</a>
).</p>

<p><code>bash Installing RStudio using apt-get
sudo apt-get install http://download1.rstudio.org/rstudio-0.97.320-amd64.deb
</code></p>

<h2>RedHat-based Distributions</h2>

<h3>RedHat EL6 (or CentOS 6+)</h3>

<p>In order to get R running on RHEL 6, we'll need to add an additional repository
that allows us to install the new packages, EPEL.  <a href="https://fedoraproject.org/wiki/EPEL">Extra Packages for
Enterprise Linux</a> (or EPEL) is a Fedora
Special Interest Group that creates, maintains, and manages a high quality set
of additional packages for Enterprise Linux, including, but not limited to, Red
Hat Enterprise Linux (RHEL), CentOS and Scientific Linux (SL).</p>

<p>``` bash Installing EPEL and R</p>

<h1>For El5 or CentOS 5</h1>

<p>su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm'
sudo yum update
sudo yum install R</p>

<h1>For El6 or CentOS 6</h1>

<p>su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm'
sudo yum update
sudo yum install R
```</p>

<p>Finally, you can search for additional R packages in terminal using <code>yum</code>:
<code>bash Searching for R packages using yum
yum list R-\*
</code></p>

<h3>Fedora</h3>

<p>For Fedora, life is much easier.  Current versions of Fedora have an up-to-date build of R in their repositories.</p>

<p><code>bash Installing R in Fedora
sudo yum update
sudo yum install R
</code></p>

<h3>Installing RStudio</h3>

<p>You'll want to copy the appropriate link for your system from the <a href="http://www.rstudio.com/ide/download/desktop">RStudio Desktop Download</a> page.  In my case, it was <code>http://download1.rstudio.org/rstudio-0.97.320-x86_64.rpm</code>.</p>

<p><code>bash Installing RStudio on Fedora/RHEL/CentOS
sudo yum install http://download1.rstudio.org/rstudio-0.97.320-x86_64.rpm
</code></p>

<h1>Dealing with PPC-based Systems</h1>

<p>As our lab has a lot of old iMacs that are still quite useful, I've recently
dealt with the best way to support R on these machines.  These machines are
still quite useful, particulary when their maximum potential RAM is installed.
However, Apple stopped supporting the machines after OS 10.5.8 (<a href="https://en.wikipedia.org/wiki/Apple%27s_transition_to_Intel_processors">see here</a>).
While the <a href="http://r.research.att.com/">nightly packages</a> distributed do install
on PPC machines with OS X, they lack the R Framework compiled for PPC, meaning
that it's a useless installation.  This means that a user stuck on OS X 10.5.8
is tied to R 2.10, a very old R distribution that's incompatible with many
existing packages.</p>

<h2>Any solutions?</h2>

<p>My first solution was to compile R from source using
<a href="https://www.macports.org/">MacPorts</a>, a package manager similar in concept to
<code>yum</code> or <code>apt-get</code>.  While successful, it takes a <em>long</em>, <em>long</em>, time to build
R and its necessary dependencies on a 1.8 Ghz G5 processor.  From a system
administrator's perspective, this also is the least parsimonious solution
possible, since each machine has to be updated with each new release of R.</p>

<p>Thanks to <a href="http://www.rstudio.com/ide/docs/server/getting_started">RStudio Server</a>, each machine
doesn't need to have R installed, as it can be run off a more powerful server
and accessed using a reasonably up-to-date browser.  I was able to install R and
RStudio on our RedHat EL6 server easily.  The trick was to make this as seemless
as possible from the user's perspective.  To accomplish this, I saved the
Bookmark to the Desktop.</p>

<p><img class="right" src="/images/ppc-r-bookmark.png" title="R Bookmark" ></p>

<p>Next, I downloaded a large R icon using Google Images and edited the Bookmark's
icon to appear as if it were R.  To do this, just copy the R icon from within
Preview, select the icon of the Bookmark by right-clicking and selecting Get
Info, and pasting using <code>Command+V</code>.</p>

<p><img class="left" src="/images/ppc-r-preview.png" width="300" title="R Preview" ></p>

<p><img class="left" src="/images/ppc-r-getinfo.png" title="Get Info" ></p>

<p>Finally, this was dragged to the OS X dock, appearing just as if it were R on the local machine, but without all the hassle and slow load times on PPC.</p>

<p><img class="right" src="/images/ppc-r-dock.png" title="Fake Dock Icon" ></p>

<p>It is worth noting that this solution is only important for users tied to OS X
10.5.8.  I've had great success with using <a href="https://fedoraproject.org/wiki/Architectures/PowerPC?rd=Arch:PPC">Fedora's PPC build</a>, which
has packages compiled for PPC already.  However, as Linux is intimidating for
many users, I chose to install RStudio Server on our lab server.</p>

<h1>Frequently Asked Questions</h1>

<ol>
<li><p>I get an error that says I'm <code>not in the sudoers file</code>:  This means that you don't have access to install software on your machine.  Talk to your system administrator.</p></li>
<li><p>I don't know if I'm running Ubuntu or Fedora.  How do I know which instructions to use?  Jokes aside, run <code>lsb_release -irc</code> in your terminal.</p></li>
</ol>

]]></content>
  </entry>
  
</feed>
